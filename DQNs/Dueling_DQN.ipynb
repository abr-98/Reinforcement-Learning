{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dueling DQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBArNXxSur4m"
      },
      "source": [
        "### Deuling DQN is another modified version of DQN, \n",
        "\n",
        "### As we have previously seen, for DQN, we try to predict the Q[s,a] or the Q value for an action on a state\n",
        "\n",
        "### The Q[s,a] value is the evaluation for the same.\n",
        "\n",
        "### Dueling DQN, decomposes the Q[s,a] function into two components V[s] and A[s,a]\n",
        "\n",
        "### V[s] is the value function of the state, on which we are taking the action in general, i.e, expected sum of the future rewards \n",
        "### from the state.\n",
        "\n",
        "### A[s,a] is the advantage function of an action on a state, so it shows how better is the action on the state over all other action.\n",
        "\n",
        "### A[s,a] = Q[s,a] - V[s]\n",
        "### or, Q[s,a] = V[s] + A[s,a]\n",
        "\n",
        "### So, we use the same DQN structure to have two branches, one that predicts the value function V[s], and other that predicts the advantage\n",
        "### function.\n",
        "\n",
        "### we have a base network, common to both the estimators parameterized w and for the value function branch, the parameters are x, and Advantage \n",
        "### branch the parameter z. The value branch will have a single output node, while the advantage will have one node per action.\n",
        "\n",
        "### According to the authors, it serves two purposes\n",
        "\n",
        "### 1. Sometimes it is not required to evaluate all the actions on a state, only the value function of the state is required. In such cases, \n",
        "### this arrangement is required\n",
        "\n",
        "### 2. Sometimes, when we predict Q values directly, as in DQN, it may often create a bias for one of the actions on a state, if the agent sees that action \n",
        "### multiple times, which may contain some bias or noise. In case of Dueling DQN, we predict V[s] which is same for all the actions on the state, so bias is\n",
        "### eliminated\n",
        "\n",
        "### Equation:\n",
        "\n",
        "### Q[s,a: w, x, z] = V[s: w, x] + A[s,a: w, z]\n",
        "\n",
        "### There was an issue with this approach, as Q[s,a] was a combination of V and A, it is impossible to decompose the value into two unique values for A and V.\n",
        "### This is called the unidentifiable problem.\n",
        "\n",
        "### The authors states that, if we can force the maximum Q value to be the V value, it may solve the problem.\n",
        "\n",
        "### Q[s,a: w, x, z] = V[s: w, x] + (A[s,a: w, z] - max(A[s, a': w, z]) for all a' belonging to the action space.\n",
        "\n",
        "### So, we make the advantage of the best action equal to 0, and the lesser the more negative value.\n",
        "\n",
        "### Later that was modified from max to mean. So, the equation was modified to:\n",
        "\n",
        "### Q[s,a: w, x, z] = V[s: w, x] + (A[s,a: w, z] - (1/|A|)*(A[s, a': w, z]) for all a' belonging to the action space.\n",
        "\n",
        "### For Dueling DQN, every thing remains similar to DQN, only model definition is modified."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3F13uxY3yoC"
      },
      "source": [
        "### Dueling DQN on cartpole"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL4Pb2cJhbij"
      },
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from memory_module import replayBuffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ4ACwuY37lI"
      },
      "source": [
        "#### instantiating environment\n",
        "#### instantiating replay-buffer to 100k samples size\n",
        "\n",
        "env=gym.make('CartPole-v0')\n",
        "env._max_episode_steps=400\n",
        "memory=replayBuffer(100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA0x8DbW4SnU"
      },
      "source": [
        "class Dueling_DQN:\n",
        "\n",
        "  def __init__(self,env,buffer):\n",
        "    self.env=env\n",
        "    self.buffer=buffer    ### Replay buffer \n",
        "    self.state_dimension=env.observation_space.shape   ### Input state dimension\n",
        "    self.no_of_action=env.action_space.n              ### No of actions\n",
        "    self.learning_rate=0.01\n",
        "    self.gamma=0.99\n",
        "    self.optimizer=tf.keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
        "    self.train_DQN=None         #### Tranining network\n",
        "    self.fixed_DQN=None         #### Oracle network\n",
        "  \n",
        "  def get_model(self):\n",
        "    ### Q = f(s,a: w)\n",
        "\n",
        "    state_input=tf.keras.layers.Input(self.state_dimension,name=\"State_input\")  ### state input\n",
        "\n",
        "    action_input=tf.keras.layers.Input((self.no_of_action,),name=\"Action_input\") ### Action input\n",
        "\n",
        "    net=tf.keras.layers.Dense(256,activation='relu')(state_input)\n",
        "    net=tf.keras.layers.Dense(256,activation='relu')(net)\n",
        "    \n",
        "    value_out=tf.keras.layers.Dense(1,name=\"Value_function\")(net)   ### Value function output,\n",
        "    ### So, the output layer has a single node, for the entire state.\n",
        "\n",
        "    advantage_out=tf.keras.layers.Dense(self.no_of_action, name=\"Advantage_function\")(net) \n",
        "    ### Advantage function output ### One node for each action \n",
        "\n",
        "    normalized_adv=tf.add(advantage_out, -tf.math.reduce_mean(advantage_out, axis=1,keepdims=True))\n",
        "    ### (A[s,a: w, z] - (1/|A|)*(A[s, a': w, z]) is achieved using the above equation\n",
        "\n",
        "    Q_all=tf.add(value_out,normalized_adv)\n",
        "    ### Q[s,a] = V[s] + A[s,a] \n",
        "    ### Q_all is for all actions output\n",
        "\n",
        "    Q_vals=tf.multiply(Q_all,action_input)\n",
        "    ### Selecting the Q-values for the required actions    \n",
        "\n",
        "    model=tf.keras.Model(inputs=[state_input,action_input],outputs=[Q_vals],name=\"Dueling_DQN\")\n",
        "    ### Creating the model\n",
        "\n",
        "    ### array of the Q values is the final output of the model.\n",
        "\n",
        "    model.compile(loss=\"mse\",optimizer=self.optimizer)\n",
        "\n",
        "    ### as we want to minimize (Q[s,a]-Q'[s,a : w])^2 we use MSE.\n",
        "\n",
        "    return model\n",
        "\n",
        "  def update_fixed(self):\n",
        "    self.fixed_DQN.set_weights(self.train_DQN.get_weights())\n",
        "    ### We will need to update the target or fixed networks with the trainee networks weight \n",
        "    ### after a few epochs.\n",
        "\n",
        "  def get_epsilon(self,episode,steady_epsilon=0.01,steady_episode=100000):\n",
        "    #### Getting the epilon for the the greedy epsilon policy, \n",
        "\n",
        "    ### epsilon linearly decays till the steady step and then becomes constant\n",
        "    if episode>steady_episode:  ##If we are above the steady episode, we return a steady epsilon\n",
        "      return steady_epsilon\n",
        "    else:\n",
        "      slope=(steady_epsilon - 1)/(steady_episode - 0) \n",
        "      ### Line (1,0) to (steady_epsilon,steady_episode)\n",
        "\n",
        "      ### slope*episode will give us the decrease in the value of epsilon\n",
        "      ### To get the value we add 1 to the value so it is (1 - decrease), as epsilon starts from 1.\n",
        "      return slope*episode + 1\n",
        "  \n",
        "  def get_action(self,state,epsilon):\n",
        "\n",
        "    if np.random.random()<epsilon:\n",
        "      return np.random.randint(self.no_of_action)\n",
        "      ### choosing random action with probability epsilon/|actions| for each.\n",
        "    else:\n",
        "      ### State is given in the shape: array([-0.0213599 , -0.03238987, -0.0356761 , -0.0347844 ])\n",
        "      ### as a 1D array, for each shape, we need to reshape it to provide a 2D array like:\n",
        "      ### array([[-0.0213599 , -0.03238987, -0.0356761 , -0.0347844 ]])\n",
        "      reshaped_state=state.reshape(1,-1)\n",
        "      \n",
        "      ### We need to pick the action which provides maximum action. To get all actions Q values, we need\n",
        "      ### to send 1 for all the actions. so in this case, the action input to the model should be: [1,1]\n",
        "\n",
        "      action_input=np.ones((1,self.no_of_action))\n",
        "      action_probs=self.train_DQN.predict([reshaped_state,action_input])\n",
        "\n",
        "      ### Action_probs has dimension 2: [[a1, a2]] as: array([[-0.00160907, -0.00242554]], dtype=float32)\n",
        "\n",
        "      ### We need to take the maximum of the of the results of the actions. so, we take np.argmax()\n",
        "      ### But we take on the axis=1 as: \n",
        "      ### in case there are mini-batches it is required to get the action for all the predictions.\n",
        "\n",
        "      ### array([[-0.00242554, -0.00160907]], dtype=float32) for this action \n",
        "      ### np.argmax(res_2,axis=0) => 1\n",
        "\n",
        "      ### array([[-0.00160907, -0.00242554],\n",
        "      ###  [-0.00242554, -0.00160907]], dtype=float32) -> for this prediction\n",
        "      ### np.argmax(res_2,axis=0) => 0   while,\n",
        "      ### np.argmax(res_2,axis=1) => [0,1], so we take on axis =1\n",
        "\n",
        "      optimal_action=np.argmax(action_probs,axis=1)[0]\n",
        "\n",
        "      return optimal_action\n",
        "\n",
        "  def on_batch(self,s,a,r,s_,not_done,gamma=0.99):\n",
        "\n",
        "    ### batch inputs\n",
        "    batch_size=s.shape[0]\n",
        "\n",
        "    ## if s is of dimension (50,4). 50 is the batch size.\n",
        "    ### as we know in q function, we take the maximum of the Q values for all the functions in the next_state.\n",
        "\n",
        "    ### same as get_Action function, but here already in shape (,4) no need to reshape.\n",
        "    ### the Q function is set using the target or fixed DQN.\n",
        "\n",
        "    action_probs=self.fixed_DQN.predict([s_,np.ones((batch_size,self.no_of_action))])\n",
        "    ## Now the Q target\n",
        "    q_targets= r + gamma*np.multiply(not_done,np.max(action_probs,axis=1))\n",
        "    ### Updated Q targets for all the states, and all the actions.\n",
        "    ### If done, not done=0, for that state, only the rewards are considered.\n",
        "\n",
        "    #### Q_targets is of the shape [v1, v2, v3.... vn]  ### where v1 is the q value updated, for that state.\n",
        "    ### but to train the network, we need it in format, [[0,v1],[v2,0]...] considering for 1st sample, action 1\n",
        "    ### was selected by the model, i.e, the value must be corresponding to the action for the state.\n",
        "\n",
        "    q_target_formatted=np.multiply(q_targets.reshape(-1,1),tf.keras.utils.to_categorical(a,self.no_of_action))\n",
        "    self.train_DQN.train_on_batch([s,tf.keras.utils.to_categorical(a,self.no_of_action)],q_target_formatted)\n",
        "    ### Training for the state on which the action is taken.\n",
        "  \n",
        "  def get_experience(self):\n",
        "\n",
        "    curr_state=self.env.reset()\n",
        "    for _ in range(50000):  \n",
        "      ### Creating 50k steps in experience to start the initial training\n",
        "\n",
        "      act=self.env.action_space.sample()   ### initially we randomly sample from the action space.\n",
        "      next_state,reward,done,_=self.env.step(act) ### Taking actions\n",
        "      self.buffer.push(curr_state,act,reward,next_state,not done)  ### Recording the details in buffer.\n",
        "\n",
        "      if done:\n",
        "        curr_state=self.env.reset()   ### If done is 1, environment is reset.\n",
        "      else:\n",
        "        curr_state=next_state        ### state is updated.\n",
        "    \n",
        "  def train(self):\n",
        "    self.train_DQN=self.get_model()\n",
        "    self.fixed_DQN=self.get_model()\n",
        "    self.get_experience()\n",
        "    ### All Initialization steps done\n",
        "    episode_reward=0\n",
        "    no_of_comp=0\n",
        "    curr_state=self.env.reset()\n",
        "    for step in range(1000000):\n",
        "      ### training on 1M steps\n",
        "      act=self.get_action(curr_state,self.get_epsilon(step))  #### getting action according to current epsilon, and state\n",
        "      next_state,reward,done,_=self.env.step(act) ### Taking the action\n",
        "      episode_reward+=reward  ## updating the reward for the step\n",
        "    \n",
        "      self.buffer.push(curr_state,act,reward,next_state,not done)  ### Pushing the details in the buffer.\n",
        "      ### Size of the buffer is fixed. so it works on LRU or first in first out policy.\n",
        "      \n",
        "      if done:\n",
        "\n",
        "        curr_state=self.env.reset()\n",
        "        if no_of_comp%50==0:\n",
        "          print('On step {}, no. of complete episodes {} episode reward {}'.format(step,no_of_comp,episode_reward))\n",
        "        episode_reward=0  ### Updating the reward to 0\n",
        "        no_of_comp+=1\n",
        "      \n",
        "      else:\n",
        "        curr_state=next_state\n",
        "\n",
        "      if step%5000==0:    ### after 5000 steps the fixed or target DQN is updated.\n",
        "        self.update_fixed()\n",
        "      \n",
        "      if step%4==0:    ### after training for 4 steps on the batch we sample new batch.\n",
        "        s,a,r,s_,nd=self.buffer.sample(32)\n",
        "        self.on_batch(s,a,r,s_,nd)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "undxW93fEVig",
        "outputId": "4354e7be-c062-4b9c-ca02-b7c51d37f58d"
      },
      "source": [
        "dqn=Dueling_DQN(env,memory)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKZ98kokEblr",
        "outputId": "93ed8cb7-24e1-4615-d526-17d8bdc7df8a"
      },
      "source": [
        "dqn.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On step 22, no. of complete episodes 0 episode reward 23.0\n",
            "On step 1201, no. of complete episodes 50 episode reward 12.0\n",
            "On step 2363, no. of complete episodes 100 episode reward 16.0\n",
            "On step 3465, no. of complete episodes 150 episode reward 21.0\n",
            "On step 4380, no. of complete episodes 200 episode reward 15.0\n",
            "On step 5340, no. of complete episodes 250 episode reward 15.0\n",
            "On step 6234, no. of complete episodes 300 episode reward 17.0\n",
            "On step 7277, no. of complete episodes 350 episode reward 36.0\n",
            "On step 8313, no. of complete episodes 400 episode reward 17.0\n",
            "On step 9344, no. of complete episodes 450 episode reward 15.0\n",
            "On step 10390, no. of complete episodes 500 episode reward 10.0\n",
            "On step 11434, no. of complete episodes 550 episode reward 62.0\n",
            "On step 12489, no. of complete episodes 600 episode reward 22.0\n",
            "On step 13602, no. of complete episodes 650 episode reward 20.0\n",
            "On step 15017, no. of complete episodes 700 episode reward 41.0\n",
            "On step 16475, no. of complete episodes 750 episode reward 33.0\n",
            "On step 18232, no. of complete episodes 800 episode reward 16.0\n",
            "On step 19739, no. of complete episodes 850 episode reward 20.0\n",
            "On step 21283, no. of complete episodes 900 episode reward 14.0\n",
            "On step 23014, no. of complete episodes 950 episode reward 105.0\n",
            "On step 25174, no. of complete episodes 1000 episode reward 79.0\n",
            "On step 27174, no. of complete episodes 1050 episode reward 18.0\n",
            "On step 29956, no. of complete episodes 1100 episode reward 20.0\n",
            "On step 32917, no. of complete episodes 1150 episode reward 214.0\n",
            "On step 35582, no. of complete episodes 1200 episode reward 29.0\n",
            "On step 38596, no. of complete episodes 1250 episode reward 125.0\n",
            "On step 41902, no. of complete episodes 1300 episode reward 17.0\n",
            "On step 46288, no. of complete episodes 1350 episode reward 37.0\n",
            "On step 52066, no. of complete episodes 1400 episode reward 220.0\n",
            "On step 59877, no. of complete episodes 1450 episode reward 189.0\n",
            "On step 69153, no. of complete episodes 1500 episode reward 298.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx2gNVUSEeJK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}