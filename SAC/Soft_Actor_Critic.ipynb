{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Soft Actor Critic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6b7L2uZbMLx",
        "outputId": "cd5fcf1e-9514-45c6-a30d-51b2ca7167a7"
      },
      "source": [
        "!pip install roboschool==1.0.48 gym==0.15.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting roboschool==1.0.48\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/31/ce69340a0698e85de2db787023aee5c9416d4ab2ded8cbccf97168ceec81/roboschool-1.0.48-cp37-cp37m-manylinux1_x86_64.whl (44.9MB)\n",
            "\u001b[K     |████████████████████████████████| 44.9MB 156kB/s \n",
            "\u001b[?25hCollecting gym==0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/85/a7a462d7796f097027d60f9a62b4e17a0a94dcf12ac2a9f9a913333b11a6/gym-0.15.4.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 35.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.15.0)\n",
            "Collecting pyglet<=1.3.2,>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 23.2MB/s \n",
            "\u001b[?25hCollecting cloudpickle~=1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.4-cp37-none-any.whl size=1648486 sha256=888c4829d16b8672e2b477244f5668d61fb55d1093145363ad7de9fe368050b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/26/9b/8a1a6599a91077a938ac4348cc3d3ac84bfab0dbfddeb4c6e7\n",
            "Successfully built gym\n",
            "\u001b[31mERROR: tensorflow-probability 0.13.0 has requirement cloudpickle>=1.3, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyglet, cloudpickle, gym, roboschool\n",
            "  Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed cloudpickle-1.2.2 gym-0.15.4 pyglet-1.3.2 roboschool-1.0.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh9XlJwRhWT8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "import roboschool\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1JDF8y8hc-d"
      },
      "source": [
        "class SAC:\n",
        "\n",
        "  def __init__(self,env,memory):\n",
        "\n",
        "      self.env=env\n",
        "      self.state_dimension=env.observation_space.shape\n",
        "      self.action_dimension=env.action_space.shape[0]\n",
        "      self.Train_actor=None\n",
        "      self.Train_critic_1=None\n",
        "      self.Target_critic_1=None\n",
        "      self.Train_critic_2=None\n",
        "      self.Target_critic_2=None\n",
        "      self.memory=memory\n",
        "      self.batch_size=256\n",
        "      self.collect_initial_=10000\n",
        "      self.cr_1_opt=tf.keras.optimizers.Adam(0.0003)\n",
        "      self.cr_2_opt=tf.keras.optimizers.Adam(0.0003)\n",
        "      self.ac_opt=tf.keras.optimizers.Adam(0.0003) \n",
        "      self.steps_to_train=1000000\n",
        "      self.tau=0.005\n",
        "      self.alpha=0.2\n",
        "      self.MAX_CLIP_sigma=2\n",
        "      self.MIN_CLIP_sigma=-20\n",
        "      self.gamma=0.99\n",
        "  \n",
        "  def get_critic(self):\n",
        "\n",
        "    input_state=tf.keras.layers.Input(self.state_dimension)\n",
        "    input_action=tf.keras.layers.Input(self.action_dimension)\n",
        "    layer_1=tf.keras.layers.concatenate([input_state,input_action],axis=-1)\n",
        "    layer_2=tf.keras.layers.Dense(256,activation=\"relu\")(layer_1)\n",
        "    layer_3=tf.keras.layers.Dense(256,activation=\"relu\")(layer_2)\n",
        "    out_Q=tf.keras.layers.Dense(1,activation=None)(layer_3)\n",
        "\n",
        "    model=tf.keras.Model(inputs=[input_state,input_action],outputs=[out_Q])\n",
        "    return model\n",
        "\n",
        "  def get_actor(self):\n",
        "\n",
        "    input=tf.keras.layers.Input(self.state_dimension)\n",
        "    layer_1=tf.keras.layers.Dense(256,activation=\"relu\")(input)\n",
        "    layer_2=tf.keras.layers.Dense(256,activation=\"relu\")(layer_1)\n",
        "    mu=tf.keras.layers.Dense(self.action_dimension,activation=None)(layer_2)\n",
        "    log_sigma=tf.keras.layers.Dense(self.action_dimension,activation=None)(layer_2)\n",
        "\n",
        "    model=tf.keras.Model(inputs=[input],outputs=[mu,log_sigma])\n",
        "    return model\n",
        "  \n",
        "  def get_action(self,actor,s):\n",
        "\n",
        "    mu,log_sigma=actor(s)\n",
        "    sigma=tf.exp(tf.clip_by_value(log_sigma,self.MIN_CLIP_sigma,self.MAX_CLIP_sigma))\n",
        "    dist=tfp.distributions.Normal(mu,sigma)\n",
        "    action=dist.sample()\n",
        "    bounded_action=tf.tanh(action)\n",
        "\n",
        "    return action,bounded_action\n",
        "  \n",
        "  def get_log_prob(self,actor,s,a,bounded_a):\n",
        "\n",
        "    mu,log_sigma=actor(s)\n",
        "    sigma=tf.exp(tf.clip_by_value(log_sigma,self.MIN_CLIP_sigma,self.MAX_CLIP_sigma))\n",
        "    dist=tfp.distributions.Normal(mu,sigma)\n",
        "    log_pr=dist.log_prob(a)\n",
        "    ### Modifying the log_pr for the action as mentioned in appendix c of the paper\n",
        "    log_pr_mod=log_pr - tf.reduce_mean(tf.math.log(1-bounded_a**2+1e-8),axis=1,keepdims=True)\n",
        "\n",
        "    return log_pr_mod\n",
        "  \n",
        "  def get_Q_value(self,critic,s,a):\n",
        "\n",
        "    q=critic([s,a])\n",
        "    return q\n",
        "\n",
        "  def initialize_buffer(self):\n",
        "    \n",
        "    curr_state=self.env.reset()\n",
        "    for _ in range(10000):\n",
        "      action=self.env.action_space.sample()\n",
        "      next_state,reward,done,_=self.env.step(action)\n",
        "      self.memory.push(curr_state,action,reward,next_state,not done)\n",
        "\n",
        "      if done:\n",
        "        curr_state=self.env.reset()\n",
        "      else:\n",
        "        curr_state=next_state\n",
        "  \n",
        "  def update_networks(self,target_net,train_net,tau):\n",
        "\n",
        "    weights_tar, weights_tra = target_net.get_weights(), train_net.get_weights()\n",
        "    for i in range(len(weights_tar)):\n",
        "      weights_tar[i] = tau*weights_tra[i] + (1-tau)*weights_tar[i]\n",
        "    target_net.set_weights(weights_tar)\n",
        "\n",
        "  def train(self):\n",
        "\n",
        "    self.Train_actor=self.get_actor()\n",
        "    self.Train_critic_1=self.get_critic()\n",
        "    self.Target_critic_1=self.get_critic()\n",
        "    self.Target_critic_1.set_weights(self.Train_critic_1.get_weights())\n",
        "    self.Train_critic_2=self.get_critic()\n",
        "    self.Target_critic_2=self.get_critic()\n",
        "    self.Target_critic_2.set_weights(self.Train_critic_2.get_weights())\n",
        "\n",
        "    self.initialize_buffer()\n",
        "\n",
        "    curr_state=self.env.reset()\n",
        "    overall_Reward=0\n",
        "    episode_reward=0\n",
        "    no_of_comp=0\n",
        "\n",
        "    for i in range(self.steps_to_train):\n",
        "      \n",
        "      action,b_action=self.get_action(self.Train_actor,curr_state.reshape(1,-1))\n",
        "      next_state,reward,done,_=self.env.step(b_action.numpy()[0])\n",
        "      episode_reward+=reward\n",
        "\n",
        "      self.memory.push(curr_state,b_action,reward,next_state,not done)\n",
        "\n",
        "      if done:\n",
        "\n",
        "          curr_state=self.env.reset()\n",
        "          overall_Reward+=episode_reward\n",
        "          if no_of_comp%20==0:\n",
        "            print('On step {}, no. of complete episodes {} average episode reward {}'.format(i,no_of_comp,overall_Reward/20))\n",
        "            overall_Reward=0\n",
        "          episode_reward=0  ### Updating the reward to 0\n",
        "          no_of_comp+=1\n",
        "    \n",
        "      else:\n",
        "        \n",
        "        curr_state=next_state\n",
        "\n",
        "      states, actions, rewards, next_states, not_dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "      with tf.GradientTape() as t1, tf.GradientTape() as t2:\n",
        "\n",
        "        n_actions,b_n_actions=self.get_action(self.Train_actor,next_states)\n",
        "        log_pr=self.get_log_prob(self.Train_actor,next_states,n_actions,b_n_actions)\n",
        "\n",
        "        next_value_1=self.get_Q_value(self.Target_critic_1,next_states,b_n_actions)\n",
        "        next_value_2=self.get_Q_value(self.Target_critic_2,next_states,b_n_actions)\n",
        "\n",
        "        pred_value_1=(self.get_Q_value(self.Train_critic_1,np.array(states,dtype=\"float32\"),np.array(actions,dtype=\"float32\")))\n",
        "        pred_value_2=(self.get_Q_value(self.Train_critic_2,np.array(states,dtype=\"float32\"),np.array(actions,dtype=\"float32\")))\n",
        "\n",
        "        next_value=tf.math.minimum(next_value_1,next_value_2)\n",
        "\n",
        "        target_value= rewards + self.gamma*(next_value - self.alpha * log_pr)*not_dones\n",
        "\n",
        "        critic_loss_1=tf.reduce_mean(tf.math.squared_difference(target_value,pred_value_1))\n",
        "        critic_loss_2=tf.reduce_mean(tf.math.squared_difference(target_value,pred_value_2))\n",
        "\n",
        "      grad_crit_1=t1.gradient(critic_loss_1,self.Train_critic_1.trainable_variables)\n",
        "      grad_crit_2=t2.gradient(critic_loss_2,self.Train_critic_2.trainable_variables)\n",
        "\n",
        "      self.cr_1_opt.apply_gradients(zip(grad_crit_1,self.Train_critic_1.trainable_variables))\n",
        "      self.cr_2_opt.apply_gradients(zip(grad_crit_2,self.Train_critic_2.trainable_variables))\n",
        "\n",
        "      with tf.GradientTape() as t:\n",
        "\n",
        "        n_actions,b_n_actions=self.get_action(self.Train_actor,states)\n",
        "        log_pr_a=self.get_log_prob(self.Train_actor,states,n_actions,b_n_actions)\n",
        "\n",
        "        state_value_1=self.get_Q_value(self.Train_critic_1,states,b_n_actions)\n",
        "        state_value_2=self.get_Q_value(self.Train_critic_2,states,b_n_actions)\n",
        "\n",
        "        value=tf.math.minimum(state_value_1,state_value_2) - self.alpha * log_pr_a\n",
        "\n",
        "        act_loss= -1*tf.reduce_mean(value)\n",
        "\n",
        "      grad_actor=t.gradient(act_loss,self.Train_actor.trainable_variables)\n",
        "      self.ac_opt.apply_gradients(zip(grad_actor,self.Train_actor.trainable_variables))\n",
        "\n",
        "      self.update_networks(self.Target_critic_1,self.Train_critic_1,self.tau)\n",
        "      self.update_networks(self.Target_critic_2,self.Train_critic_2,self.tau)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_na3S4hkcr5"
      },
      "source": [
        "env = gym.make('RoboschoolInvertedPendulum-v1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBktcW1ykj6p"
      },
      "source": [
        "from memory_module import replayBuffer\n",
        "memory=replayBuffer(100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDALmsxJk6Dp"
      },
      "source": [
        "agent=SAC(env,memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m99Gg0UUk9zs"
      },
      "source": [
        "agent.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-8Y7XXalApQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}