{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3PG_scratch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "fjc6tEuaA5Ro",
        "outputId": "a63c2218-ba30-4853-e700-ba4a23127117"
      },
      "source": [
        "!pip install roboschool==1.0.48 gym==0.15.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting roboschool==1.0.48\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/31/ce69340a0698e85de2db787023aee5c9416d4ab2ded8cbccf97168ceec81/roboschool-1.0.48-cp37-cp37m-manylinux1_x86_64.whl (44.9MB)\n",
            "\u001b[K     |████████████████████████████████| 44.9MB 165kB/s \n",
            "\u001b[?25hCollecting gym==0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/85/a7a462d7796f097027d60f9a62b4e17a0a94dcf12ac2a9f9a913333b11a6/gym-0.15.4.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 40.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.15.0)\n",
            "Collecting pyglet<=1.3.2,>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 25.8MB/s \n",
            "\u001b[?25hCollecting cloudpickle~=1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.4-cp37-none-any.whl size=1648486 sha256=44fb8813af3730973199882a12b484f53f5eb3bf2fa6ed7dc069217fbb5251af\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/26/9b/8a1a6599a91077a938ac4348cc3d3ac84bfab0dbfddeb4c6e7\n",
            "Successfully built gym\n",
            "\u001b[31mERROR: tensorflow-probability 0.13.0 has requirement cloudpickle>=1.3, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyglet, cloudpickle, gym, roboschool\n",
            "  Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed cloudpickle-1.2.2 gym-0.15.4 pyglet-1.3.2 roboschool-1.0.48\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cloudpickle",
                  "gym"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0FXcKwWvefW"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTL4fh8oA3Ds"
      },
      "source": [
        "import gym\n",
        "import roboschool"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "089XZClJBYCp"
      },
      "source": [
        "env = gym.make('RoboschoolInvertedPendulum-v1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6U6frB9A_-U"
      },
      "source": [
        "state_dimension=env.observation_space.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8D1W9FpBjNa"
      },
      "source": [
        "action_dimension=env.action_space.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DTrP6W6Boe_"
      },
      "source": [
        "min_action=env.action_space.low[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru0bA7sDBwuH"
      },
      "source": [
        "max_action=env.action_space.high[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go_MZcEPB0x4"
      },
      "source": [
        "def get_critic():\n",
        "\n",
        "  input_state=tf.keras.layers.Input(state_dimension)\n",
        "  input_action=tf.keras.layers.Input(action_dimension)\n",
        "  layer_1=tf.keras.layers.concatenate([input_state,input_action],axis=-1)\n",
        "  layer_2=tf.keras.layers.Dense(400,activation=\"relu\")(layer_1)\n",
        "  layer_3=tf.keras.layers.Dense(300,activation=\"relu\")(layer_2)\n",
        "  out_Q=tf.keras.layers.Dense(1,activation=None)(layer_3)\n",
        "\n",
        "  model=tf.keras.Model(inputs=[input_state,input_action],outputs=[out_Q])\n",
        "  return model\n",
        "\n",
        "def get_actor():\n",
        "\n",
        "  input=tf.keras.layers.Input(state_dimension)\n",
        "  layer_1=tf.keras.layers.Dense(400,activation=\"relu\")(input)\n",
        "  layer_2=tf.keras.layers.Dense(300,activation=\"relu\")(layer_1)\n",
        "  out=tf.keras.layers.Dense(action_dimension,activation=\"tanh\")(layer_2)\n",
        "\n",
        "  model=tf.keras.Model(inputs=[input],outputs=[out])\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByqXaUZhLx8_"
      },
      "source": [
        "Train_actor=get_actor()\n",
        "Target_actor=get_actor()\n",
        "Target_actor.set_weights(Train_actor.get_weights())\n",
        "Train_critic_1=get_critic()\n",
        "Target_critic_1=get_critic()\n",
        "Target_critic_1.set_weights(Train_critic_1.get_weights())\n",
        "Train_critic_2=get_critic()\n",
        "Target_critic_2=get_critic()\n",
        "Target_critic_2.set_weights(Train_critic_2.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty0m21EYNEDv"
      },
      "source": [
        "from memory_module import replayBuffer\n",
        "memory=replayBuffer(100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxq3wgKWOM-v"
      },
      "source": [
        "def get_action(actor,s,sigma=0,noise=False):\n",
        "  mu=actor(s)\n",
        "  Noise_sigma=sigma\n",
        "  if noise:\n",
        "    action=mu+tf.random.normal(shape=[action_dimension],mean=0,stddev=Noise_sigma)\n",
        "  else:\n",
        "    action=mu\n",
        "\n",
        "  action=max_action*(tf.clip_by_value(action,min_action,max_action))  ## AS tanh is used in activation\n",
        "  return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WFa8SElUelW"
      },
      "source": [
        "def get_Q_value(critic,s,a):\n",
        "  q=critic([s,a])\n",
        "  return q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac-VSw1KgExY"
      },
      "source": [
        "def initialize_buffer():\n",
        "    \n",
        "    curr_state=env.reset()\n",
        "    for _ in range(10000):\n",
        "      action=env.action_space.sample()\n",
        "      next_state,reward,done,_=env.step(action)\n",
        "      memory.push(curr_state,action,reward,next_state,not done)\n",
        "\n",
        "      if done:\n",
        "        curr_state=env.reset()\n",
        "      else:\n",
        "        curr_state=next_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3TajdApusnz"
      },
      "source": [
        "initialize_buffer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt2bnAHS1Sc1"
      },
      "source": [
        "cr_1_opt=tf.keras.optimizers.Adam(0.001)\n",
        "cr_2_opt=tf.keras.optimizers.Adam(0.001)\n",
        "ac_opt=tf.keras.optimizers.Adam(0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNbYpJ_LFZfw"
      },
      "source": [
        "def update_networks(target_net,train_net,tau):\n",
        "  weights_tar, weights_tra = target_net.get_weights(), train_net.get_weights()\n",
        "  for i in range(len(weights_tar)):\n",
        "    weights_tar[i] = tau*weights_tra[i] + (1-tau)*weights_tar[i]\n",
        "  target_net.set_weights(weights_tar)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajxDoBfn17My"
      },
      "source": [
        "def critic_pred(critic,states):\n",
        "\n",
        "  c=0.5\n",
        "  mu=Target_actor(states)\n",
        "  noise_action=mu+tf.clip_by_value(tf.random.normal(shape=[action_dimension],mean=0,stddev=0.2),-c,c)\n",
        "  predicted_actions=max_action*tf.clip_by_value(noise_action,min_action,max_action)\n",
        "\n",
        "  next_state_value=get_Q_value(critic,states,predicted_actions)\n",
        "  return next_state_value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9TOEn8k64p2"
      },
      "source": [
        "def loss_critics(states, actions, rewards, next_states, not_dones, gamma=0.99):\n",
        "  next_value_1=tf.squeeze(critic_pred(Target_critic_1,next_states))\n",
        "  next_value_2=tf.squeeze(critic_pred(Target_critic_2,next_states))\n",
        "\n",
        "  pred_value_1=tf.squeeze(get_Q_value(Train_critic_1,np.array(states,dtype=\"float32\"),np.array(actions,dtype=\"float32\")))\n",
        "  pred_value_2=tf.squeeze(get_Q_value(Train_critic_2,np.array(states,dtype=\"float32\"),np.array(actions,dtype=\"float32\")))\n",
        "\n",
        "  next_value=tf.math.minimum(next_value_1,next_value_2)\n",
        "\n",
        "  target_value= rewards + gamma*next_value*not_dones\n",
        "\n",
        "  critic_loss_1=tf.reduce_mean(tf.math.squared_difference(target_value,pred_value_1))\n",
        "  critic_loss_2=tf.reduce_mean(tf.math.squared_difference(target_value,pred_value_2))\n",
        "\n",
        "  return critic_loss_1,critic_loss_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bt4oKN9u0xK"
      },
      "source": [
        "def train():\n",
        "\n",
        "  batch_size=128\n",
        "  steps_to_train=1000000\n",
        "  update_actor_step=2\n",
        "  tau=0.005\n",
        "  curr_state=env.reset()\n",
        "  overall_Reward=0\n",
        "  episode_reward=0\n",
        "  no_of_comp=0\n",
        "  steps_to_stop_exp=2000\n",
        "  for i in range(steps_to_train):\n",
        "    \n",
        "    if i<steps_to_stop_exp:\n",
        "      action=get_action(Train_actor,curr_state.reshape(1,-1),sigma=0.1,noise=True)\n",
        "    else:\n",
        "      action=get_action(Train_actor,curr_state.reshape(1,-1))\n",
        "    next_state,reward,done,_=env.step(action.numpy()[0])\n",
        "    episode_reward+=reward\n",
        "\n",
        "    memory.push(curr_state,action,reward,next_state,not done)\n",
        "\n",
        "    if done:\n",
        "\n",
        "        curr_state=env.reset()\n",
        "        overall_Reward+=episode_reward\n",
        "        if no_of_comp%20==0:\n",
        "          print('On step {}, no. of complete episodes {} average episode reward {}'.format(i,no_of_comp,overall_Reward/20))\n",
        "          overall_Reward=0\n",
        "        episode_reward=0  ### Updating the reward to 0\n",
        "        no_of_comp+=1\n",
        "    else:\n",
        "      curr_state=next_state\n",
        "\n",
        "    states, actions, rewards, next_states, not_dones = memory.sample(batch_size)\n",
        "\n",
        "    with tf.GradientTape() as t1, tf.GradientTape() as t2:\n",
        "      critic_loss_1,critic_loss_2=loss_critics(states, actions, rewards, next_states, not_dones)\n",
        "\n",
        "    grad_crit_1=t1.gradient(critic_loss_1,Train_critic_1.trainable_variables)\n",
        "    grad_crit_2=t2.gradient(critic_loss_2,Train_critic_2.trainable_variables)\n",
        "\n",
        "    cr_1_opt.apply_gradients(zip(grad_crit_1,Train_critic_1.trainable_variables))\n",
        "    cr_2_opt.apply_gradients(zip(grad_crit_2,Train_critic_2.trainable_variables))\n",
        "\n",
        "    if i % update_actor_step==0:\n",
        "\n",
        "      with tf.GradientTape() as t:\n",
        "        new_actions=Train_actor(states)\n",
        "        act_loss=-1*tf.reduce_mean(Train_critic_1([states,new_actions]))\n",
        "\n",
        "      grad_act=t.gradient(act_loss,Train_actor.trainable_variables)\n",
        "      ac_opt.apply_gradients(zip(grad_act,Train_actor.trainable_variables))\n",
        "\n",
        "      update_networks(Target_actor,Train_actor,tau)\n",
        "      update_networks(Target_critic_1,Train_critic_1,tau)\n",
        "      update_networks(Target_critic_2,Train_critic_2,tau)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utZE2uFUCUi2",
        "outputId": "cf33f577-c032-4ed4-a5f3-08e27fca99c7"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/memory_module.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(s),np.array(a),np.array(r),np.array(s_),np.uint8(nd)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "On step 9, no. of complete episodes 0 average episode reward 0.5\n",
            "On step 190, no. of complete episodes 20 average episode reward 9.05\n",
            "On step 375, no. of complete episodes 40 average episode reward 9.25\n",
            "On step 545, no. of complete episodes 60 average episode reward 8.5\n",
            "On step 741, no. of complete episodes 80 average episode reward 9.8\n",
            "On step 927, no. of complete episodes 100 average episode reward 9.3\n",
            "On step 1109, no. of complete episodes 120 average episode reward 9.1\n",
            "On step 1292, no. of complete episodes 140 average episode reward 9.15\n",
            "On step 1473, no. of complete episodes 160 average episode reward 9.05\n",
            "On step 1664, no. of complete episodes 180 average episode reward 9.55\n",
            "On step 1855, no. of complete episodes 200 average episode reward 9.55\n",
            "On step 2638, no. of complete episodes 220 average episode reward 39.15\n",
            "On step 4816, no. of complete episodes 240 average episode reward 108.9\n",
            "On step 6597, no. of complete episodes 260 average episode reward 89.05\n",
            "On step 8313, no. of complete episodes 280 average episode reward 85.8\n",
            "On step 10258, no. of complete episodes 300 average episode reward 97.25\n",
            "On step 12264, no. of complete episodes 320 average episode reward 100.3\n",
            "On step 14431, no. of complete episodes 340 average episode reward 108.35\n",
            "On step 16623, no. of complete episodes 360 average episode reward 109.6\n",
            "On step 20256, no. of complete episodes 380 average episode reward 181.65\n",
            "On step 23405, no. of complete episodes 400 average episode reward 157.45\n",
            "On step 25492, no. of complete episodes 420 average episode reward 104.35\n",
            "On step 27706, no. of complete episodes 440 average episode reward 110.7\n",
            "On step 30302, no. of complete episodes 460 average episode reward 129.8\n",
            "On step 41681, no. of complete episodes 480 average episode reward 568.95\n",
            "On step 58459, no. of complete episodes 500 average episode reward 838.9\n",
            "On step 70003, no. of complete episodes 520 average episode reward 577.2\n",
            "On step 87662, no. of complete episodes 540 average episode reward 882.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFQI1HEOCfAN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}