{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO_continous_action_space_multi-environment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0yBPbMeN_ng"
      },
      "source": [
        "### Render"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWdmUXrkNooG"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX-odiG5NwKu"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs4FNJ8pNzvR"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVHXktX0N1nA",
        "outputId": "a7cbef8b-08ae-4591-99c4-588e2bd47e98"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fb78bcc3390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "0rgTWeU7N4kg",
        "outputId": "5dddf4c6-5647-4bf8-99fd-43eeb24e522d"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(50):\n",
        "  action = env.action_space.sample()\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  screen = env.render(mode='rgb_array')\n",
        "\n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "SugJeq2uOM2C",
        "outputId": "a81eb062-d906-477c-cd7c-78d82d685a3a"
      },
      "source": [
        "!pip install roboschool==1.0.48 gym==0.15.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting roboschool==1.0.48\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/31/ce69340a0698e85de2db787023aee5c9416d4ab2ded8cbccf97168ceec81/roboschool-1.0.48-cp37-cp37m-manylinux1_x86_64.whl (44.9MB)\n",
            "\u001b[K     |████████████████████████████████| 44.9MB 150kB/s \n",
            "\u001b[?25hCollecting gym==0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/85/a7a462d7796f097027d60f9a62b4e17a0a94dcf12ac2a9f9a913333b11a6/gym-0.15.4.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 25.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.15.0)\n",
            "Collecting pyglet<=1.3.2,>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 31.5MB/s \n",
            "\u001b[?25hCollecting cloudpickle~=1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.4-cp37-none-any.whl size=1648486 sha256=18dc0194814776e1513358bd25f4c77858c790a52b513e5b804e4571435275bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/26/9b/8a1a6599a91077a938ac4348cc3d3ac84bfab0dbfddeb4c6e7\n",
            "Successfully built gym\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement cloudpickle>=1.3, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyglet, cloudpickle, gym, roboschool\n",
            "  Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed cloudpickle-1.2.2 gym-0.15.4 pyglet-1.3.2 roboschool-1.0.48\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cloudpickle",
                  "gym"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmD8VLK14c9X"
      },
      "source": [
        "!git clone https://github.com/Stable-Baselines-Team/stable-baselines-tf2.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThryqB6x4ehL",
        "outputId": "7b92f244-1a7f-46e5-eb32-a3b12e179746"
      },
      "source": [
        "!pip install ./stable-baselines-tf2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing collected packages: stable-baselines\n",
            "Successfully installed stable-baselines-3.0.0a0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEaaFi465uuc"
      },
      "source": [
        "### The A2C algorithms are very less sample efficient, and hence is unsuitable for solving continous action space problems."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhDaxw3pLav7"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import pandas as pd\n",
        "import functools\n",
        "import operator\n",
        "import gym\n",
        "import roboschool\n",
        "from multiprocessing_env import SubprocVecEnv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsQ6e9Oi9zih"
      },
      "source": [
        "### The pi_old is the poicy for the n-1 th iteration policy, when we calculate for n th iteration.\n",
        "### The constraint is only applied for a particuar iteration for the batch updates."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP9-ojhSLuOz"
      },
      "source": [
        "class PPO_continous_action:\n",
        "\n",
        "  def __init__(self,env,env_name,num_of_env,env_max_steps=None):\n",
        "\n",
        "    self.env=env\n",
        "    self.state_dimension=env.observation_space.shape   ###PPO_continous_action_space Input state dimension\n",
        "    self.no_of_action=env.action_space.shape[0]              ### No of actions\n",
        "    self.no_of_env=num_of_env\n",
        "    self.Actor=None      ### the learner\n",
        "    self.Critic=None     ### Critic\n",
        "    self.env=SubprocVecEnv([self.create_env(env_name,i,max_steps=env_max_steps) for i in range(self.no_of_env)])\n",
        "    self.opt=tf.keras.optimizers.Adam(0.0003)\n",
        "    self.steps_in_epi=512    ### Fixed number of steps in a episode\n",
        "    self.iterations=5000         ### Number of epochs\n",
        "    #self.log_std=tf.Variable(tf.zeros((1,self.no_of_action)))\n",
        "    self.max_clip_value=2\n",
        "    self.min_clip_value=-20\n",
        "    self.epochs=10\n",
        "\n",
        "  def create_env(self,env_id,rank=0,max_steps=None):\n",
        "      seed=100\n",
        "      def get_env():         #### The seed lets generate same sequence, and starting state is same and\n",
        "      ### every time performance is same\n",
        "        env=gym.make(env_id)\n",
        "        if max_steps is not None:\n",
        "          env._max_episode_steps=max_steps\n",
        "        env.seed(seed+rank)\n",
        "        return env\n",
        "      return get_env\n",
        "    ### Creates the multiple environments\n",
        "\n",
        "  \n",
        "  def get_actor(self):\n",
        "\n",
        "    input_layer=tf.keras.layers.Input(self.state_dimension)  ### Takes the state for which we want to \n",
        "    ### predict the probability distribution of the actions.\n",
        "\n",
        "    layer_1=tf.keras.layers.Dense(128,activation=\"relu\")(input_layer)\n",
        "    layer_2=tf.keras.layers.Dense(128,activation=\"relu\")(layer_1)\n",
        "    layer_3=tf.keras.layers.Dense(128,activation=\"relu\")(layer_2)\n",
        "    \n",
        "    output_layer_mu=tf.keras.layers.Dense(self.no_of_action,activation=None)(layer_3)\n",
        "    output_layer_log_sigma=tf.keras.layers.Dense(self.no_of_action,activation=None)(layer_3)\n",
        "\n",
        "    ### Predicts the peobability of all the actions on the state s, so the number of nodes in\n",
        "    ### the final layer of model is equal to the number of actions \n",
        "    ### and we generate the mean action of the state so, we use a relu instead of softmax in case of discrete actions\n",
        "    \n",
        "    model=tf.keras.Model(inputs=[input_layer],outputs=[output_layer_mu,output_layer_log_sigma])\n",
        "    \n",
        "    return model\n",
        "  \n",
        "  def get_critic(self):\n",
        "\n",
        "    input_layer=tf.keras.layers.Input(self.state_dimension)  ### Takes the state for which we want to \n",
        "    ### predict the estimate value function V(s)\n",
        "\n",
        "    layer_1=tf.keras.layers.Dense(128,activation=\"relu\")(input_layer)\n",
        "    layer_2=tf.keras.layers.Dense(128,activation=\"relu\")(layer_1)\n",
        "    layer_3=tf.keras.layers.Dense(128,activation=\"relu\")(layer_2)\n",
        "    \n",
        "    output_layer=tf.keras.layers.Dense(1)(layer_3)  ### Predicts the Value function for that state.\n",
        "\n",
        "    model=tf.keras.Model(inputs=[input_layer],outputs=[output_layer])\n",
        "    \n",
        "    return model\n",
        "  \n",
        "  \n",
        "  def action(self,s):\n",
        "    \n",
        "\n",
        "    mu,log_sigma=self.Actor(s)   \n",
        "    action_prob=tfp.distributions.Normal(mu,tf.math.exp(tf.clip_by_value(log_sigma,self.min_clip_value,self.max_clip_value)))\n",
        "    action= action_prob.sample()\n",
        "    ### sampling an action from the obtained probability distributions for all the action\n",
        "    return action.numpy()  ### Action returned as 1D tensor-> converting to scalar\n",
        "\n",
        "  def prob(self,s,a):\n",
        "\n",
        "    mu,log_sigma=self.Actor(s)\n",
        "    ### again to be used in batch\n",
        "    action_prob=tfp.distributions.Normal(mu,tf.math.exp(tf.clip_by_value(log_sigma,self.min_clip_value,self.max_clip_value)))\n",
        "    ### takes in the states and the actions and returns the corresponding log probability \n",
        "    ### of the occurence of the taken action a on the state s\n",
        "    ### log(P[a|s : w]) is obtained.\n",
        "    return action_prob.log_prob(a)\n",
        "  \n",
        "  def get_value(self,state):\n",
        "\n",
        "    value=self.Critic(state).numpy()\n",
        "    return value\n",
        "\n",
        "  def get_test_action(self,s):\n",
        "    s=s.reshape(1,-1)\n",
        "    mean_action,_=self.Actor(s)\n",
        "    return mean_action.numpy()\n",
        "\n",
        "  def actor_loss(self,log_old_probs,S,A,Adv,epsilon=0.2):\n",
        "    \n",
        "    log_new_probs=self.prob(S,A)  \n",
        "    importance_ratio = tf.math.exp(tf.math.subtract(log_new_probs,log_old_probs))\n",
        "    surr_1=tf.multiply(importance_ratio,Adv)\n",
        "    surr_2=tf.multiply(tf.clip_by_value(importance_ratio,1-epsilon,1+epsilon),Adv)\n",
        "    L_clip=-1*tf.reduce_mean(tf.reduce_min([surr_1,surr_2],axis=0))\n",
        "    \n",
        "    return L_clip\n",
        "\n",
        "  def critic_loss(self,S,ret):\n",
        "    L_vf= 0.5*tf.reduce_mean(tf.math.squared_difference(ret,self.Critic(S)))\n",
        "    return L_vf\n",
        "  \n",
        "  def entropy(self,S):\n",
        "    mu,log_sigma=self.Actor(S)\n",
        "    dist=tfp.distributions.Normal(mu,tf.math.exp(tf.clip_by_value(log_sigma,self.min_clip_value,self.max_clip_value)))\n",
        "    L_S=tf.reduce_mean(dist.entropy())\n",
        "    return L_S\n",
        "    ### Entropy induces exploration.\n",
        "  \n",
        "  def total_loss(self,old_probs,S,A,rets,Adv,c1=0.5,c2=0.001):\n",
        "    act_loss=self.actor_loss(old_probs,S,A,Adv)\n",
        "    crit_loss=self.critic_loss(S,rets)\n",
        "    entropy_loss=self.entropy(S)\n",
        "\n",
        "    total_loss=act_loss+c1*crit_loss-c2*entropy_loss\n",
        "    return total_loss\n",
        "  \n",
        "  def train_on_batch(self,probs,s,a,adv,r):\n",
        "    with tf.GradientTape() as t:\n",
        "        loss=self.total_loss(probs,s,a,r,adv)\n",
        "      ### Calculating loss\n",
        "    grads=t.gradient(loss,self.Actor.trainable_variables+self.Critic.trainable_variables)\n",
        "\n",
        "    self.opt.apply_gradients(zip(grads,self.Actor.trainable_variables+self.Critic.trainable_variables))\n",
        "    #### Applying gradients.\n",
        "    return 0\n",
        "  \n",
        "  def get_episodes(self):\n",
        "    ### To sample raw walks in the environment\n",
        "    states=[]\n",
        "    actions=[]\n",
        "    rewards=[]\n",
        "    values=[]\n",
        "    next_values=[]\n",
        "    not_done=[]\n",
        "    old_probs=[]\n",
        "    \n",
        "\n",
        "    done=False\n",
        "    curr_state=self.env.reset()\n",
        "    for _ in range(self.steps_in_epi):\n",
        "      ### Recording fixed number of steps.\n",
        "      acts=self.action(curr_state)\n",
        "      next_state,reward,done,_=self.env.step(acts)\n",
        "      ### Obtaining next step.\n",
        "      value=self.get_value(curr_state)\n",
        "      next_value=self.get_value(next_state)\n",
        "      probs=self.prob(curr_state,acts)\n",
        "\n",
        "      states.append(curr_state)\n",
        "      actions.append(acts)\n",
        "      rewards.append(reward)\n",
        "      values.append(value.ravel())\n",
        "      next_values.append(next_value.ravel())\n",
        "      old_probs.append(probs)\n",
        "      not_done.append(1-done)\n",
        "\n",
        "      curr_state=next_state\n",
        "      \n",
        "      ### logging the essential required values for loss estimation\n",
        "\n",
        "    return np.asarray(states).astype(np.float32),\\\n",
        "           np.asarray(actions).astype(np.float32),\\\n",
        "           np.asarray(rewards).astype(np.float32),\\\n",
        "           np.asarray(values).astype(np.float32),\\\n",
        "           np.asarray(next_values).astype(np.float32),\\\n",
        "           np.asarray(old_probs).astype(np.float32),\\\n",
        "           np.asarray(not_done).astype(np.float32)  ### All logs returned\n",
        "\n",
        "  def get_gae(self,next_values,values,rewards,not_dones,gamma=0.99,lam=0.95):\n",
        "    \n",
        "    gae=0\n",
        "    returns=[]\n",
        "    ### Calculating GAE according to formulation.\n",
        "    for step in reversed(range(len(rewards))):\n",
        "      TD_error_delta = rewards[step] + gamma * next_values[step] * not_dones[step] - values[step]\n",
        "      gae = TD_error_delta + gamma * lam * not_dones[step] * gae\n",
        "      ret = gae + values[step]\n",
        "      returns.insert(0,ret)\n",
        "    \n",
        "    return np.array(returns)\n",
        "    \n",
        "  def functools_reduce_iconcat(self,a):\n",
        "    return np.array(functools.reduce(operator.iconcat, a, []),dtype=\"float32\")\n",
        "    ### Converts stacks of the environment returns to 1D vectors\n",
        "\n",
        " \n",
        "  def test_play(self):\n",
        "    ### Testing results for current weights.\n",
        "    overall=0\n",
        "    for _ in range(5):\n",
        "      curr_states=self.env.reset()\n",
        "      curr_state=curr_states[0]\n",
        "      total_reward=0\n",
        "      done=[False]*self.no_of_env\n",
        "      while not done[0]:\n",
        "        a=self.get_test_action(curr_state)\n",
        "        acts=[a[0]]\n",
        "        for z in range(self.no_of_env-1):\n",
        "          acts.append(np.array([0]*self.no_of_action,dtype=\"float32\"))\n",
        "        next_state,reward,done,_=(self.env.step(np.array(acts)))\n",
        "        total_reward+=reward[0]\n",
        "        curr_state=next_state[0]\n",
        "\n",
        "      overall+=total_reward\n",
        "    return overall/5\n",
        "  \n",
        "  def train(self,batch_size=128):\n",
        "    ### Training\n",
        "    self.Actor=self.get_actor()\n",
        "    self.Critic=self.get_critic()\n",
        "\n",
        "    for i in range(self.iterations):\n",
        "      ### For each epoch\n",
        "      s,a,R,v,Nv,op,Nd=self.get_episodes()\n",
        "      \n",
        "      ret=self.get_gae(Nv,v,R,Nd)\n",
        "\n",
        "      ret=self.functools_reduce_iconcat(ret).reshape(-1,1)\n",
        "      s=self.functools_reduce_iconcat(s)\n",
        "      a=self.functools_reduce_iconcat(a)\n",
        "      v=self.functools_reduce_iconcat(v).reshape(-1,1)\n",
        "      op=self.functools_reduce_iconcat(op)\n",
        "\n",
        "      adv=ret-v\n",
        "      adv=adv.astype('float32')\n",
        "      adv=(adv-adv.mean())/(adv.std())\n",
        "      total_no_of_samples=len(s)\n",
        "\n",
        "\n",
        "      dataset=tf.data.Dataset.from_tensor_slices((op,s,a,adv,ret)).shuffle(total_no_of_samples).repeat(self.epochs).batch(batch_size,drop_remainder=True)         \n",
        "\n",
        "      for (prob_sample,s_sample,a_sample,adv_sample,r_sample) in dataset:\n",
        "        self.train_on_batch(prob_sample,s_sample,a_sample,adv_sample,r_sample)\n",
        "      \n",
        "      if i%10==0:\n",
        "        score=self.test_play()\n",
        "        print(f\"On Iteration {i} scores: {score}\")\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlkJAkUrOyd5"
      },
      "source": [
        "env = gym.make('RoboschoolInvertedPendulum-v1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH2bnTadO2aC"
      },
      "source": [
        "agent=PPO_continous_action(env,'RoboschoolInvertedPendulum-v1',8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUoqKBXhO5Ro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96783274-7fbb-4e45-97eb-a07fb9349e0a"
      },
      "source": [
        "agent.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On Iteration 0 scores: 162.2\n",
            "On Iteration 10 scores: 1000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4FaO91GCkMZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}