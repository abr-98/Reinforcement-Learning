{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Double DQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHL4V8QOLkBH"
      },
      "source": [
        "### The DQN algorithm always considers the action, on a particular state, as the best action which has the maximum\n",
        "### value for the Q function. \n",
        "\n",
        "### This optimistic approach creates a maximization bias already as discussed. \n",
        "\n",
        "### As during the update, we are updating an estimate from a given estimate, this adds up enough noise, and using the \n",
        "### same network for action selection and action evaluation creates a high chance of a bias creation. \n",
        "\n",
        "### Often it is seen that, initially it is seen, that because of the noise, the action having the maximum Q-value function \n",
        "### is not best action taken at that point, the best actions, may have a low value of Q function.\n",
        "\n",
        "### To reduce the bias, we use two different network, so technically we decouple the action selection and action evaluation \n",
        "### so one of the network, is used to select the action for a state, using argmax or a greedy policy, and another network is \n",
        "### used for evaluating the action and updating the Q-function.\n",
        "\n",
        "### This way as two networks are involved, the estimates could be deemed unbaised, and the maximization bias is reduced.\n",
        "\n",
        "### So, equations is modified to:\n",
        "\n",
        "### Q[st,at : w] = Q[st,at : w] + alpha * (rt + gamma * Q'[s(t+1), a: w'] - Q[st,at: w])\n",
        "\n",
        "### a= argmax(Q[s(t+1),:w]) with exploration ### The action that provides the maximum value for Q-value for all action on that state, estimated by w parametered function\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmd4csXy8hEP"
      },
      "source": [
        "### The implementatio is almost similar to DQN, only during the action selection the code changes and during the updation.\n",
        "\n",
        "### For the oracle network, the action is produced by the train dqn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv9J4gkmh1zN"
      },
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from memory_module import replayBuffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8-Vtad4iKey"
      },
      "source": [
        "#### instantiating environment\n",
        "#### instantiating replay-buffer to 100k samples size\n",
        "\n",
        "env=gym.make('CartPole-v0')\n",
        "env._max_episode_steps=400\n",
        "memory=replayBuffer(100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILTfdbbxiMpq"
      },
      "source": [
        "class Double_DQN:\n",
        " \n",
        "  def __init__(self,env,buffer):\n",
        "    self.env=env\n",
        "    self.buffer=buffer    ### Replay buffer \n",
        "    self.state_dimension=env.observation_space.shape   ### Input state dimension\n",
        "    self.no_of_action=env.action_space.n              ### No of actions\n",
        "    self.learning_rate=0.01\n",
        "    self.gamma=0.99\n",
        "    self.optimizer=tf.keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
        "    self.train_DQN=None         #### Tranining network\n",
        "    self.fixed_DQN=None         #### Oracle network\n",
        "  \n",
        "  def get_model(self):\n",
        "    ### Q = f(s,a: w)\n",
        "\n",
        "    state_input=tf.keras.layers.Input(self.state_dimension,name=\"State_input\")  ### state input\n",
        "\n",
        "    action_input=tf.keras.layers.Input((self.no_of_action,),name=\"Action_input\") ### Action input\n",
        "\n",
        "    net=tf.keras.layers.Dense(256,activation='relu')(state_input)\n",
        "    net=tf.keras.layers.Dense(256,activation='relu')(net)\n",
        "    output=tf.keras.layers.Dense(self.no_of_action,name=\"function_out\")(net)\n",
        "\n",
        "    ### So, the model takes in the state representation as input and produces the Q values for the all the actions\n",
        "    ### Then for each action, given by: action 1: [0 1], the [0 1] is multiplied with the output of the model in form [a1,a2]\n",
        "    ### to get the output of corresponding to the action required. [a1, a2].[0, 1] = [0, a2]\n",
        "\n",
        "    Q_values=tf.multiply(output,action_input, name=\"masking_output\")\n",
        "\n",
        "    model=tf.keras.Model(inputs=[state_input,action_input],outputs=[Q_values],name=\"DQN\")\n",
        "\n",
        "    ### array of the Q values is the final output of the model.\n",
        "\n",
        "    model.compile(loss=\"mse\",optimizer=self.optimizer)\n",
        "\n",
        "    ### as we want to minimize (Q[s,a]-Q'[s,a : w])^2 we use MSE.\n",
        "\n",
        "    return model\n",
        "  \n",
        "  def update_fixed(self):\n",
        "    self.fixed_DQN.set_weights(self.train_DQN.get_weights())\n",
        "    ### We will need to update the target or fixed networks with the trainee networks weight \n",
        "    ### after a few epochs.\n",
        "  \n",
        "  def get_epsilon(self,episode,steady_epsilon=0.01,steady_episode=100000):\n",
        "    #### Getting the epilon for the the greedy epsilon policy, \n",
        "\n",
        "    ### epsilon linearly decays till the steady step and then becomes constant\n",
        "    if episode>steady_episode:  ##If we are above the steady episode, we return a steady epsilon\n",
        "      return steady_epsilon\n",
        "    else:\n",
        "      slope=(steady_epsilon - 1)/(steady_episode - 0) \n",
        "      ### Line (1,0) to (steady_epsilon,steady_episode)\n",
        "\n",
        "      ### slope*episode will give us the decrease in the value of epsilon\n",
        "      ### To get the value we add 1 to the value so it is (1 - decrease), as epsilon starts from 1.\n",
        "      return slope*episode + 1\n",
        "\n",
        "  def get_action(self,state,epsilon):\n",
        "\n",
        "    if np.random.random()<epsilon:\n",
        "      return np.random.randint(self.no_of_action)\n",
        "      ### choosing random action with probability epsilon/|actions| for each.\n",
        "    else:\n",
        "      ### State is given in the shape: array([-0.0213599 , -0.03238987, -0.0356761 , -0.0347844 ])\n",
        "      ### as a 1D array, for each shape, we need to reshape it to provide a 2D array like:\n",
        "      ### array([[-0.0213599 , -0.03238987, -0.0356761 , -0.0347844 ]])\n",
        "      reshaped_state=state.reshape(1,-1)\n",
        "      \n",
        "      ### We need to pick the action which provides maximum action. To get all actions Q values, we need\n",
        "      ### to send 1 for all the actions. so in this case, the action input to the model should be: [1,1]\n",
        "\n",
        "      action_input=np.ones((1,self.no_of_action))\n",
        "      action_probs=self.train_DQN.predict([reshaped_state,action_input])\n",
        "\n",
        "      ### Action_probs has dimension 2: [[a1, a2]] as: array([[-0.00160907, -0.00242554]], dtype=float32)\n",
        "\n",
        "      ### We need to take the maximum of the of the results of the actions. so, we take np.argmax()\n",
        "      ### But we take on the axis=1 as: \n",
        "      ### in case there are mini-batches it is required to get the action for all the predictions.\n",
        "\n",
        "      ### array([[-0.00242554, -0.00160907]], dtype=float32) for this action \n",
        "      ### np.argmax(res_2,axis=0) => 1\n",
        "\n",
        "      ### array([[-0.00160907, -0.00242554],\n",
        "      ###  [-0.00242554, -0.00160907]], dtype=float32) -> for this prediction\n",
        "      ### np.argmax(res_2,axis=0) => 0   while,\n",
        "      ### np.argmax(res_2,axis=1) => [0,1], so we take on axis =1\n",
        "\n",
        "      optimal_action=np.argmax(action_probs,axis=1)[0]\n",
        "\n",
        "      return optimal_action\n",
        "\n",
        "  def on_batch(self,s,a,r,s_,not_done,step,gamma=0.99):\n",
        "\n",
        "    ### batch inputs\n",
        "    batch_size=s.shape[0]\n",
        "\n",
        "    ## if s is of dimension (50,4). 50 is the batch size.\n",
        "    ### as we know in q function, we take the maximum of the Q values for all the functions in the next_state.\n",
        "\n",
        "    ### same as get_Action function, but here already in shape (,4) no need to reshape.\n",
        "    ### the Q function is set using the target or fixed DQN.\n",
        "\n",
        "    q_values=[]\n",
        "    for i in range(batch_size):    ### For each sample in the batch\n",
        "      a_=self.get_action(s_[i],self.get_epsilon(step))     ### obtaining the actions for the \n",
        "      ### next state, using the training DQN (action selection)\n",
        "      q=self.fixed_DQN.predict([s_[i].reshape(1,-1),(np.array(tf.keras.utils.to_categorical(a_,self.no_of_action))).reshape(1,-1)])\n",
        "      ### picking the q values for the next state and the predicted actions from the fixed DQN (action evaluation)\n",
        "      q_values.append(q[0][a_])   ### Maintaining the q value for the predicted actions for all the states in the batch\n",
        "\n",
        "      \n",
        "\n",
        "    ## Now the Q target\n",
        "    q_targets= r + gamma*np.multiply(not_done,np.array(q_values))\n",
        "    ### Updated Q targets for all the states, and all the actions.\n",
        "    ### If done, not done=0, for that state, only the rewards are considered.\n",
        "\n",
        "    #### Q_targets is of the shape [v1, v2, v3.... vn]  ### where v1 is the q value updated, for that state.\n",
        "    ### but to train the network, we need it in format, [[0,v1],[v2,0]...] considering for 1st sample, action 1\n",
        "    ### was selected by the model, i.e, the value must be corresponding to the action for the state.\n",
        "\n",
        "    q_target_formatted=np.multiply(q_targets.reshape(-1,1),tf.keras.utils.to_categorical(a,self.no_of_action))\n",
        "    self.train_DQN.train_on_batch([s,tf.keras.utils.to_categorical(a,self.no_of_action)],q_target_formatted)\n",
        "    ### Training for the state on which the action is taken.\n",
        "\n",
        "  def get_experience(self):\n",
        "\n",
        "    curr_state=self.env.reset()\n",
        "    for _ in range(50000):  \n",
        "      ### Creating 50k steps in experience to start the initial training\n",
        "\n",
        "      act=self.env.action_space.sample()   ### initially we randomly sample from the action space.\n",
        "      next_state,reward,done,_=self.env.step(act) ### Taking actions\n",
        "      self.buffer.push(curr_state,act,reward,next_state,not done)  ### Recording the details in buffer.\n",
        "\n",
        "      if done:\n",
        "        curr_state=self.env.reset()   ### If done is 1, environment is reset.\n",
        "      else:\n",
        "        curr_state=next_state        ### state is updated.\n",
        "\n",
        "  def train(self):\n",
        "    self.train_DQN=self.get_model()\n",
        "    self.fixed_DQN=self.get_model()\n",
        "    self.get_experience()\n",
        "    ### All Initialization steps done\n",
        "    episode_reward=0\n",
        "    no_of_comp=0\n",
        "    curr_state=self.env.reset()\n",
        "    for step in range(1000000):\n",
        "      ### training on 1M steps\n",
        "      act=self.get_action(curr_state,self.get_epsilon(step))  #### getting action according to current epsilon, and state\n",
        "      next_state,reward,done,_=self.env.step(act) ### Taking the action\n",
        "      episode_reward+=reward  ## updating the reward for the step\n",
        "    \n",
        "      self.buffer.push(curr_state,act,reward,next_state,not done)  ### Pushing the details in the buffer.\n",
        "      ### Size of the buffer is fixed. so it works on LRU or first in first out policy.\n",
        "      \n",
        "      if done:\n",
        "\n",
        "        curr_state=self.env.reset()\n",
        "        if no_of_comp%50==0:\n",
        "          print('On step {}, no. of complete episodes {} episode reward {}'.format(step,no_of_comp,episode_reward))\n",
        "        episode_reward=0  ### Updating the reward to 0\n",
        "        no_of_comp+=1\n",
        "      \n",
        "      else:\n",
        "        curr_state=next_state\n",
        "\n",
        "      if step%5000==0:    ### after 5000 steps the fixed or target DQN is updated.\n",
        "        self.update_fixed()\n",
        "      \n",
        "      if step%4==0:    ### after training for 4 steps on the batch we sample new batch.\n",
        "        s,a,r,s_,nd=self.buffer.sample(32)\n",
        "        self.on_batch(s,a,r,s_,nd,step)\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feuXu4ian97j",
        "outputId": "ebd80b5b-41d5-4dea-f287-7d5c67cca561"
      },
      "source": [
        "dqn=Double_DQN(env,memory)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ9mhBh_sWoj",
        "outputId": "ff08c9f8-99e6-4c47-a589-e288651bb9a7"
      },
      "source": [
        "dqn.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On step 49, no. of complete episodes 0 episode reward 50.0\n",
            "On step 1191, no. of complete episodes 50 episode reward 14.0\n",
            "On step 2280, no. of complete episodes 100 episode reward 55.0\n",
            "On step 3523, no. of complete episodes 150 episode reward 31.0\n",
            "On step 4649, no. of complete episodes 200 episode reward 11.0\n",
            "On step 5736, no. of complete episodes 250 episode reward 9.0\n",
            "On step 6909, no. of complete episodes 300 episode reward 26.0\n",
            "On step 8324, no. of complete episodes 350 episode reward 21.0\n",
            "On step 9550, no. of complete episodes 400 episode reward 17.0\n",
            "On step 10890, no. of complete episodes 450 episode reward 30.0\n",
            "On step 12294, no. of complete episodes 500 episode reward 10.0\n",
            "On step 13673, no. of complete episodes 550 episode reward 14.0\n",
            "On step 14934, no. of complete episodes 600 episode reward 26.0\n",
            "On step 16302, no. of complete episodes 650 episode reward 26.0\n",
            "On step 17741, no. of complete episodes 700 episode reward 48.0\n",
            "On step 19543, no. of complete episodes 750 episode reward 26.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}