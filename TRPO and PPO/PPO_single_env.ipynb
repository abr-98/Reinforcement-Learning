{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO_single_env.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhDaxw3pLav7"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gstjfl51Lrva"
      },
      "source": [
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP9-ojhSLuOz"
      },
      "source": [
        "class PPO_single_agent:\n",
        "\n",
        "  def __init__(self,env):\n",
        "\n",
        "    self.env=env\n",
        "    self.state_dimension=env.observation_space.shape   ### Input state dimension\n",
        "    self.no_of_action=env.action_space.n              ### No of actions\n",
        "    self.Actor=None      ### the learner\n",
        "    self.Critic=None     ### Critic\n",
        "    self.opt=tf.keras.optimizers.Adam(0.0003)\n",
        "    self.steps_in_epi=512    ### Fixed number of steps in a episode\n",
        "    self.epochs=5000         ### Number of epochs\n",
        "    self.m=8                 ### Number of samples in a epoch.\n",
        "    self.target=500\n",
        "  \n",
        "  def get_actor(self):\n",
        "\n",
        "    input_layer=tf.keras.layers.Input(self.state_dimension)  ### Takes the state for which we want to \n",
        "    ### predict the probability distribution of the actions.\n",
        "\n",
        "    layer_1=tf.keras.layers.Dense(128,activation=\"relu\")(input_layer)\n",
        "    layer_2=tf.keras.layers.Dense(128,activation=\"relu\")(layer_1)\n",
        "    layer_3=tf.keras.layers.Dense(128,activation=\"relu\")(layer_2)\n",
        "    \n",
        "    output_layer=tf.keras.layers.Dense(self.no_of_action, activation=\"softmax\")(layer_3)\n",
        "\n",
        "    ### Predicts the peobability of all the actions on the state s, so the number of nodes in\n",
        "    ### the final layer of model is equal to the number of actions \n",
        "    ### and we generate a probabilistic distribution so softmax is used as the activation function.\n",
        "    \n",
        "    model=tf.keras.Model(inputs=[input_layer],outputs=[output_layer])\n",
        "    \n",
        "    return model\n",
        "  \n",
        "  def get_critic(self):\n",
        "\n",
        "    input_layer=tf.keras.layers.Input(self.state_dimension)  ### Takes the state for which we want to \n",
        "    ### predict the estimate value function V(s)\n",
        "\n",
        "    layer_1=tf.keras.layers.Dense(128,activation=\"relu\")(input_layer)\n",
        "    layer_2=tf.keras.layers.Dense(128,activation=\"relu\")(layer_1)\n",
        "    layer_3=tf.keras.layers.Dense(128,activation=\"relu\")(layer_2)\n",
        "    \n",
        "    output_layer=tf.keras.layers.Dense(1)(layer_3)  ### Predicts the Value function for that state.\n",
        "\n",
        "    model=tf.keras.Model(inputs=[input_layer],outputs=[output_layer])\n",
        "    \n",
        "    return model\n",
        "  \n",
        "  \n",
        "  def action(self,s):\n",
        "    \n",
        "    s=s.reshape(1,-1)\n",
        "    out=self.Actor(s)   \n",
        "    action_prob=tfp.distributions.Categorical(probs=out)\n",
        "    action= action_prob.sample()\n",
        "    ### sampling an action from the obtained probability distributions for all the action\n",
        "    return action.numpy()[0]  ### Action returned as 1D tensor-> converting to scalar\n",
        "\n",
        "  def prob(self,s,a):\n",
        "\n",
        "    out=self.Actor(s)\n",
        "    ### again to be used in batch\n",
        "    action_prob=tfp.distributions.Categorical(probs=out)\n",
        "    ### takes in the states and the actions and returns the corresponding log probability \n",
        "    ### of the occurence of the taken action a on the state s\n",
        "    ### log(P[a|s : w]) is obtained.\n",
        "    return action_prob.prob(a)\n",
        "\n",
        "  def actor_loss(self,old_probs,S,A,Adv,epsilon=0.2):\n",
        "    \n",
        "    new_probs=self.prob(S,A) \n",
        "    importance_ratio = tf.divide(new_probs,old_probs)\n",
        "    surr_1=tf.multiply(importance_ratio,Adv)\n",
        "    surr_2=tf.multiply(tf.clip_by_value(importance_ratio,1-epsilon,1+epsilon),Adv)\n",
        "    L_clip=-1*tf.reduce_mean(tf.reduce_min([surr_1,surr_2],axis=0))\n",
        "    \n",
        "    return L_clip\n",
        "\n",
        "  def critic_loss(self,S,ret):\n",
        "    L_vf= 0.5*tf.reduce_mean(tf.math.squared_difference(ret,self.Critic(S)))\n",
        "    return L_vf\n",
        "  \n",
        "  def entropy(self,S):\n",
        "    logits=self.Actor(S)\n",
        "    dist=tfp.distributions.Categorical(logits=logits)\n",
        "    L_S=tf.reduce_mean(dist.entropy())\n",
        "    return L_S\n",
        "    ### Entropy induces exploration.\n",
        "  \n",
        "  def total_loss(self,old_probs,S,A,rets,Adv,c1=0.5,c2=0.001):\n",
        "    act_loss=self.actor_loss(old_probs,S,A,Adv)\n",
        "    crit_loss=self.critic_loss(S,rets)\n",
        "    entropy_loss=self.entropy(S)\n",
        "\n",
        "    total_loss=act_loss+c1*crit_loss-c2*entropy_loss\n",
        "    return total_loss\n",
        "  \n",
        "  def train_on_batch(self,probs,s,a,adv,r):\n",
        "    with tf.GradientTape() as t:\n",
        "        loss=self.total_loss(probs,s,a,r,adv)\n",
        "      ### Calculating loss\n",
        "    grads=t.gradient(loss,self.Actor.trainable_variables+self.Critic.trainable_variables)\n",
        "    self.opt.apply_gradients(zip(grads,self.Actor.trainable_variables+self.Critic.trainable_variables))\n",
        "    #### Applying gradients.\n",
        "    return 0\n",
        "  \n",
        "  def get_episodes(self):\n",
        "    ### To sample raw walks in the environment\n",
        "    states=[]\n",
        "    actions=[]\n",
        "    rewards=[]\n",
        "    next_states=[]\n",
        "    not_done=[]\n",
        "    \n",
        "\n",
        "    done=False\n",
        "    curr_state=self.env.reset()\n",
        "    for _ in range(self.steps_in_epi):\n",
        "      ### Recording fixed number of steps.\n",
        "      action=self.action(curr_state)\n",
        "      next_state,reward,done,_=self.env.step(action)\n",
        "      ### Obtaining next step.\n",
        "\n",
        "      states.append(curr_state)\n",
        "      actions.append(action)\n",
        "      rewards.append(reward)\n",
        "      next_states.append(next_state)\n",
        "      not_done.append(not done)\n",
        "      \n",
        "      ### logging the essential required values for loss estimation\n",
        "      if done:\n",
        "        curr_state=env.reset()  ### If done, every value is reset.\n",
        "        done=False\n",
        "      else: \n",
        "        curr_state=next_state ### Updating.\n",
        "\n",
        "    return states,actions,rewards,next_states,not_done  ### All logs returned\n",
        "\n",
        "  def get_value_funcs(self,states,next_states):\n",
        "    \n",
        "    #print(states)\n",
        "    values=self.Critic.predict(np.array(states))    ### Recording the values for the states \n",
        "    values= values.flatten()      ### Converting the 2D array to 1D array\n",
        "    next_values=self.Critic.predict(np.array(next_states))   ### Recording the values for the next states\n",
        "    next_values= next_values.flatten()      ### Converting the 2D array to 1D array\n",
        "    return values,next_values\n",
        "\n",
        "  def get_old_probs(self,states,actions):\n",
        "    \n",
        "    probs=self.prob(np.array(states),np.array(actions)).numpy()\n",
        "    probs=probs.flatten()\n",
        "    return np.array(probs)\n",
        "\n",
        "  def get_gae(self,next_values,values,rewards,not_dones,gamma=0.99,lam=0.95):\n",
        "    \n",
        "    gae=0\n",
        "    returns=[]\n",
        "    ### Calculating GAE according to formulation.\n",
        "    for step in reversed(range(len(rewards))):\n",
        "      TD_error_delta = rewards[step] + gamma * next_values[step] * not_dones[step] - values[step]\n",
        "      gae = TD_error_delta + gamma * lam * not_dones[step] * gae\n",
        "      ret = gae + values[step]\n",
        "      returns.insert(0,ret)\n",
        "    \n",
        "    return np.array(returns)\n",
        "\n",
        "  def get_experience(self,m):\n",
        "    #### Getting the experience for all m samplings.\n",
        "    states=[]\n",
        "    actions=[]\n",
        "    returns=[]\n",
        "    values=[]\n",
        "    old_probs=[]\n",
        "\n",
        "    for i in range(m):\n",
        "      S,A,R,Ns,Nd=self.get_episodes()\n",
        "      vals,next_vals=self.get_value_funcs(S,Ns)\n",
        "      old_prob=self.get_old_probs(S,A)\n",
        "      rets=self.get_gae(next_vals,vals,R,Nd)\n",
        "\n",
        "      \n",
        "      ### For each episode in number of samples, collecting experience\n",
        "      old_probs.extend(old_prob)\n",
        "      states.extend(S)\n",
        "      actions.extend(A)\n",
        "      returns.extend(rets)\n",
        "      values.extend(vals)\n",
        "   \n",
        "    return np.array(states),np.array(actions),np.array(returns),np.array(values),np.array(old_probs)\n",
        "  \n",
        "  def test_play(self):\n",
        "    ### Testing results for current weights.\n",
        "    overall=0\n",
        "    for _ in range(5):\n",
        "      curr_state=self.env.reset()\n",
        "      total_reward=0\n",
        "      done=False\n",
        "      while not done:\n",
        "        a=self.action(curr_state.reshape(1,-1))\n",
        "        next_state,reward,done,_=self.env.step(a)\n",
        "        total_reward+=reward\n",
        "        curr_state=next_state\n",
        "      overall+=total_reward\n",
        "    return overall/5\n",
        "  \n",
        "  def train(self,batch_size=128):\n",
        "    ### Training\n",
        "    self.Actor=self.get_actor()\n",
        "    self.Critic=self.get_critic()\n",
        "\n",
        "    for i in range(self.epochs):\n",
        "      ### For each epoch\n",
        "      s,a,r,v,op = self.get_experience(self.m)\n",
        "      #### Obtaining values.\n",
        "      adv= r - v\n",
        "      #### Calculating advantages\n",
        "      adv = (adv - adv.mean())/adv.std()\n",
        "      ### Normalizing the advantages\n",
        "      total_no_of_samples=len(s)\n",
        "\n",
        "      dataset=tf.data.Dataset.from_tensor_slices((op,s,a,adv,r)).shuffle(total_no_of_samples).batch(batch_size,drop_remainder=True)         \n",
        "      iterator=dataset.as_numpy_iterator()\n",
        "\n",
        "      no_of_batches=total_no_of_samples/batch_size\n",
        "\n",
        "      for _ in range(int(no_of_batches)):\n",
        "        prob_sample,s_sample,a_sample,adv_sample,r_sample=iterator.next()\n",
        "        self.train_on_batch(prob_sample,s_sample,a_sample,adv_sample,r_sample)\n",
        "      \n",
        "    \n",
        "      if i%10==0:\n",
        "        score=self.test_play()\n",
        "        print(f\"On Iteration {i} scores: {score}\")\n",
        "        if score==self.target:\n",
        "          break\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlkJAkUrOyd5"
      },
      "source": [
        "env=gym.make('CartPole-v0')\n",
        "env._max_episode_steps=500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH2bnTadO2aC"
      },
      "source": [
        "agent=PPO_single_agent(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUoqKBXhO5Ro",
        "outputId": "0d8976af-48f5-4492-8b94-32887ea85f96"
      },
      "source": [
        "agent.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On Iteration 0 scores: 29.6\n",
            "On Iteration 10 scores: 192.2\n",
            "On Iteration 20 scores: 326.6\n",
            "On Iteration 30 scores: 379.4\n",
            "On Iteration 40 scores: 472.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOKIxdSQO9hq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}