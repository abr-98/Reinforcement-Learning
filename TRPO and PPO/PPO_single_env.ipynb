{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO_single_env.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhDaxw3pLav7"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gstjfl51Lrva"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsQ6e9Oi9zih"
      },
      "source": [
        "### The pi_old is the poicy for the n-1 th iteration policy, when we calculate for n th iteration.\n",
        "### The constraint is only applied for a particuar iteration for the batch updates."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0XyXmAa_5Zt"
      },
      "source": [
        "### PPO trains on the same set of collected samples, using different batches, for k epochs. So, it can \n",
        "### train the models k*no of batches number of times.\n",
        "\n",
        "### The samples are updated only when, the agents move from one iteration to the other. old_policy for \n",
        "### iteration i is the policy developed after all k*no of batches number of updates of the i-1 th iteration\n",
        "### For the all the updates on the i th iteration the old policy remains constant. So the sample efficiency \n",
        "### is said to be high compared to A2C which updates only once an iteration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP9-ojhSLuOz"
      },
      "source": [
        "class PPO_single_agent:\n",
        "\n",
        "  def __init__(self,env):\n",
        "\n",
        "    self.env=env\n",
        "    self.state_dimension=env.observation_space.shape   ### Input state dimension\n",
        "    self.no_of_action=env.action_space.n              ### No of actions\n",
        "    self.Actor=None      ### the learner\n",
        "    self.Critic=None     ### Critic\n",
        "    self.opt=tf.keras.optimizers.Adam(0.0003)\n",
        "    self.steps_in_epi=512    ### Fixed number of steps in a episode\n",
        "    self.iterations=5000         ### Number of epochs\n",
        "    self.m=8                 ### Number of samples in a epoch.\n",
        "    self.target=1000\n",
        "    self.epochs=10\n",
        "  \n",
        "  def get_actor(self):\n",
        "\n",
        "    input_layer=tf.keras.layers.Input(self.state_dimension)  ### Takes the state for which we want to \n",
        "    ### predict the probability distribution of the actions.\n",
        "\n",
        "    layer_1=tf.keras.layers.Dense(128,activation=\"relu\")(input_layer)\n",
        "    layer_2=tf.keras.layers.Dense(128,activation=\"relu\")(layer_1)\n",
        "    layer_3=tf.keras.layers.Dense(128,activation=\"relu\")(layer_2)\n",
        "    \n",
        "    output_layer=tf.keras.layers.Dense(self.no_of_action, activation=\"softmax\")(layer_3)\n",
        "\n",
        "    ### Predicts the peobability of all the actions on the state s, so the number of nodes in\n",
        "    ### the final layer of model is equal to the number of actions \n",
        "    ### and we generate a probabilistic distribution so softmax is used as the activation function.\n",
        "    \n",
        "    model=tf.keras.Model(inputs=[input_layer],outputs=[output_layer])\n",
        "    \n",
        "    return model\n",
        "  \n",
        "  def get_critic(self):\n",
        "\n",
        "    input_layer=tf.keras.layers.Input(self.state_dimension)  ### Takes the state for which we want to \n",
        "    ### predict the estimate value function V(s)\n",
        "\n",
        "    layer_1=tf.keras.layers.Dense(128,activation=\"relu\")(input_layer)\n",
        "    layer_2=tf.keras.layers.Dense(128,activation=\"relu\")(layer_1)\n",
        "    layer_3=tf.keras.layers.Dense(128,activation=\"relu\")(layer_2)\n",
        "    \n",
        "    output_layer=tf.keras.layers.Dense(1)(layer_3)  ### Predicts the Value function for that state.\n",
        "\n",
        "    model=tf.keras.Model(inputs=[input_layer],outputs=[output_layer])\n",
        "    \n",
        "    return model\n",
        "  \n",
        "  \n",
        "  def get_action(self,state):\n",
        "\n",
        "    state=state.reshape(1,-1)\n",
        "    acts=self.Actor(state)\n",
        "    act_probs=tfp.distributions.Categorical(probs=acts)\n",
        "    return act_probs.sample().numpy()[0]\n",
        "  \n",
        "  def get_value(self,state):\n",
        "    state=state.reshape(1,-1)\n",
        "    value=self.Critic(state).numpy()[0]\n",
        "    return value\n",
        "\n",
        "  def get_prob(self,state,action):\n",
        "    acts=self.Actor(state)\n",
        "    act_probs=tfp.distributions.Categorical(probs=acts)\n",
        "    return act_probs.log_prob(action)\n",
        "\n",
        "  def actor_loss(self,S,A,Adv,log_old_probs,eps=0.2):\n",
        "    log_new_probs=tf.reshape(self.get_prob(S,tf.reshape(A,(1,-1))[0]),shape=(-1,1))\n",
        "    r=tf.math.exp(tf.math.subtract(log_new_probs,log_old_probs)) \n",
        "    surr_1=tf.multiply(r,Adv)\n",
        "    surr_2=tf.multiply(tf.clip_by_value(r,1-eps,1+eps),Adv)\n",
        "    l_clip=-1*tf.reduce_mean(tf.reduce_min([surr_1,surr_2],axis=0))\n",
        "    return l_clip\n",
        "\n",
        "  def critic_loss(self,S,ret):\n",
        "    l_vf= 0.5*tf.reduce_mean(tf.math.squared_difference(ret,self.Critic(S)))\n",
        "    return l_vf\n",
        "    \n",
        "  def entropy(self,S):\n",
        "      logits=self.Actor(S)\n",
        "      dist=tfp.distributions.Categorical(logits=logits)\n",
        "      L_S=tf.reduce_mean(dist.entropy())\n",
        "      return L_S\n",
        "    ### Entropy induces exploration.\n",
        "  \n",
        "  def total_loss(self,old_probs,S,A,rets,Adv,c1=0.5,c2=0.001):\n",
        "    act_loss=self.actor_loss(S,A,Adv,old_probs)\n",
        "    crit_loss=self.critic_loss(S,rets)\n",
        "    entropy_loss=self.entropy(S)\n",
        "\n",
        "    total_loss=act_loss+c1*crit_loss-c2*entropy_loss\n",
        "    return total_loss\n",
        "  \n",
        "  def train_on_batch(self,probs,s,a,adv,r):\n",
        "    with tf.GradientTape() as t:\n",
        "        loss=self.total_loss(probs,s,a,r,adv)\n",
        "      ### Calculating loss\n",
        "    grads=t.gradient(loss,self.Actor.trainable_variables+self.Critic.trainable_variables)\n",
        "    self.opt.apply_gradients(zip(grads,self.Actor.trainable_variables+self.Critic.trainable_variables))\n",
        "    #### Applying gradients.\n",
        "    return 0\n",
        "    \n",
        "  def get_episodes(self):\n",
        "    ### To sample raw walks in the environment\n",
        "    states=[]\n",
        "    actions=[]\n",
        "    rewards=[]\n",
        "    values=[]\n",
        "    next_values=[]\n",
        "    not_done=[]\n",
        "    old_probs=[]\n",
        "    \n",
        "\n",
        "    done=False\n",
        "    curr_state=self.env.reset()\n",
        "    for _ in range(self.steps_in_epi):\n",
        "      ### Recording fixed number of steps.\n",
        "      action=self.get_action(curr_state)\n",
        "      next_state,reward,done,_=self.env.step(action)\n",
        "      value=self.get_value(curr_state)\n",
        "      next_value=self.get_value(next_state)\n",
        "      prob=self.get_prob(curr_state.reshape(1,-1),action)\n",
        "      ### Obtaining next step.\n",
        "\n",
        "      states.append(curr_state)\n",
        "      actions.append([action])\n",
        "      rewards.append([reward])\n",
        "      values.append(value)\n",
        "      next_values.append(next_value)\n",
        "      not_done.append([not done])\n",
        "      old_probs.append(prob)\n",
        "      \n",
        "      ### logging the essential required values for loss estimation\n",
        "      if done:\n",
        "        curr_state=self.env.reset()  ### If done, every value is reset.\n",
        "        done=False\n",
        "      else: \n",
        "        curr_state=next_state ### Updating.\n",
        "\n",
        "    return np.array(states),np.array(actions),np.array(rewards),np.array(values),np.array(next_values),np.array(not_done),np.array(old_probs)  ### All logs returned\n",
        "\n",
        "  def get_gae(self,next_values,values,rewards,not_dones,gamma=0.99,lam=0.95):\n",
        "    \n",
        "    gae=0\n",
        "    returns=[]\n",
        "    ### Calculating GAE according to formulation.\n",
        "    for step in reversed(range(len(rewards))):\n",
        "      TD_error_delta = rewards[step] + gamma * next_values[step] * not_dones[step] - values[step]\n",
        "      gae = TD_error_delta + gamma * lam * not_dones[step] * gae\n",
        "      ret = gae + values[step]\n",
        "      returns.insert(0,ret)\n",
        "    \n",
        "    return np.array(returns)\n",
        "\n",
        "  def get_experience(self,m):\n",
        "    #### Getting the experience for all m samplings.\n",
        "    states=[]\n",
        "    actions=[]\n",
        "    returns=[]\n",
        "    values=[]\n",
        "    old_probs=[]\n",
        "\n",
        "    for i in range(m):\n",
        "      s,a,r,v,nv,nd,op=self.get_episodes()\n",
        "      ret=self.get_gae(nv,v,r,nd)\n",
        "\n",
        "      ### For each episode in number of samples, collecting experience\n",
        "      old_probs.extend(op)\n",
        "      states.extend(s)\n",
        "      actions.extend(a)\n",
        "      returns.extend(ret)\n",
        "      values.extend(v)\n",
        "   \n",
        "    return np.array(states),np.array(actions),np.array(returns),np.array(values),np.array(old_probs)\n",
        "\n",
        "  def test_play(self):\n",
        "    ### Testing results for current weights.\n",
        "    overall=0\n",
        "    for _ in range(5):\n",
        "      curr_state=self.env.reset()\n",
        "      total_reward=0\n",
        "      done=False\n",
        "      while not done:\n",
        "        a=self.get_action(curr_state.reshape(1,-1))\n",
        "        next_state,reward,done,_=self.env.step(a)\n",
        "        total_reward+=reward\n",
        "        curr_state=next_state\n",
        "      overall+=total_reward\n",
        "    return overall/5\n",
        "  \n",
        "  def train(self,batch_size=128):\n",
        "    self.Actor=self.get_actor()\n",
        "    self.Critic=self.get_critic()\n",
        "    for i in range(self.iterations):\n",
        "\n",
        "      s,a,r,v,op=self.get_experience(self.m)\n",
        "      r=r.astype('float32')\n",
        "      adv=r-v\n",
        "      adv=adv.astype('float32')\n",
        "      adv=(adv-adv.mean())/(adv.std())\n",
        "      total_no_of_samples=len(s)\n",
        "\n",
        "      dataset=tf.data.Dataset.from_tensor_slices((op,s,a,adv,r)).shuffle(total_no_of_samples).repeat(self.epochs).batch(batch_size,drop_remainder=True)         \n",
        "\n",
        "      for (prob_sample,s_sample,a_sample,adv_sample,r_sample) in dataset:\n",
        "        self.train_on_batch(prob_sample,s_sample,a_sample,adv_sample,r_sample)\n",
        "      \n",
        "    \n",
        "      if i%10==0:\n",
        "        score=self.test_play()\n",
        "        print(f\"On Iteration {i} scores: {score}\")\n",
        "        if score==self.target:\n",
        "          break\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlkJAkUrOyd5"
      },
      "source": [
        "env=gym.make('CartPole-v0')\n",
        "env._max_episode_steps=1000"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH2bnTadO2aC"
      },
      "source": [
        "agent=PPO_single_agent(env)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUoqKBXhO5Ro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15f243b2-6f52-4978-9cdc-5342ded5fc10"
      },
      "source": [
        "agent.train()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On Iteration 0 scores: 58.2\n",
            "On Iteration 10 scores: 1000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GBhRur6VtRO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}