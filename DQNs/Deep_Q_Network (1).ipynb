{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Q Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe3nDlAumlP4"
      },
      "source": [
        "### Value Function Approximation\n",
        "\n",
        "### Often the state spaces are huge and if we use table based methods, they take up enormous space to store the action-state values\n",
        "### that is infeasable, and insufficient.\n",
        "\n",
        "### In such cases, we need something for generalization, i.e, even our agent has never seen the state before, still it can make good decisions\n",
        "\n",
        "### In this cases, we will want to represent the value function, or the state-action value function as a parameterized function \n",
        "### instead of a table. \n",
        "\n",
        "## S --> f(S:W) ---> V(S)\n",
        "## S,A ---> g(S,A:W)---> Q(S,A)\n",
        "### W is the wieght parameters of the function, So,we give (State, Action) as input to the parameterized function.\n",
        "### It gives us Q[s,a]\n",
        "\n",
        "### We just try to create a general compact representation, to reduce the memory and computation required to find a good policy.\n",
        "\n",
        "### For VFA, most used algorithms like:\n",
        "\n",
        "### 1. Linear Regression \n",
        "### 2. Neural Networks\n",
        "\n",
        "## The parameterized Neural Networks predict the state-action value function Q'(s,a: w) in case of policy control and V'(s: w) in case of policy evaluation \n",
        "\n",
        "## To predict the parameterized function tries to decrease the MSE between Q(s,a) and Q'(s,a: w) in case of policy control, where Q(s,a) is the actual\n",
        "## target value of the Q function, and Q'(s,a: w) is the Q value predicted by the approximation model. We replace Q[s,a] by V[s] and every thing remains same for\n",
        "## Policy Evaluation\n",
        "\n",
        "### The loss function: J(w) = E[(Q(s,a) - Q'(s,a: w))^2] i.e, the MSE between actual and the predicted value or Q-function, for policy contol.\n",
        "### The loss function: J(w) = E[(V(s) - V'(s: w))^2] i.e, the MSE between actual and the predicted value or V-function, for policy contol.\n",
        "\n",
        "### Gradient descent is used on parameters w to minimize J(w): W = W - alpha * dJ(w)/dw.\n",
        "\n",
        "### alpha is the learning parameter. Now, this gradient descent is handled and learnt as parameterized loss function in a neural network architecture.\n",
        "\n",
        "### Now, as we have seen, to train the Neural Networks, we need a source to obtain the actual Q or V function values-> Q[s,a] or V[s]\n",
        "\n",
        "### This source is called an ORACLE. \n",
        "\n",
        "### Initially, the model was trained on a single sample, or a single state-value tuple at a time, to avoid the expectation, and so SGD was used.\n",
        "\n",
        "#### VALUE FUNCTION: FOR POLICY EVALUATION \n",
        "\n",
        "### To create this oracle, we use the estimation methods, we used previously,\n",
        "### 1. Monte-carlo based methods\n",
        "### 2. TD Learning Based methods\n",
        "\n",
        "#### STATE-ACTION VALUE FUNCTION: FOR POLICY CONTROL\n",
        "\n",
        "### To create this oracle, we use the estimation methods, we used previously,\n",
        "### 1. Monte-carlo based methods\n",
        "### 2. SARSA Based methods\n",
        "### 3. Q-leaning Based methods\n",
        "\n",
        "### The only difference is, in the update step after finding the updated V, or Q function, we have to fit it to the approximator for it to update its weight update.\n",
        "\n",
        "### Formulation:\n",
        "\n",
        "## For the formulation, states are respresenatation are represented as a vector. A state is a represented as a set of n features. These n features form a vector.\n",
        "## So, V'(s: w)= B0+ W1*X1(s)+..........WnXn(s).\n",
        "\n",
        "### => V'(s:w) = transpose(X(s)).W\n",
        "### => dV'(s:w)/dw = X(s)\n",
        "### W is the weight parameter matrix and X is the state vector.\n",
        "\n",
        "### Similarly for Q'(s,a: w)= transpose(X(s,a)).W\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1ThA6kz6OLA"
      },
      "source": [
        "#### WE UPDATE THE WEIGHTS OF THE APPROXIMATORS IN CASE OF VFA\n",
        "\n",
        "### FOR VALUE FUNCTION:\n",
        "\n",
        "### Monte-Carlo Update:\n",
        "\n",
        "### We sample ((s1,G1), (s2,G2).................(sn,Gn))... for batch training\n",
        "\n",
        "## W = W + alpha * (Gt - V'(st : w))dV'(st:w)/dw.................(1)\n",
        "\n",
        "## W = W + alpha * (Gt - transpose(X(st)).W)).X(st)................(2) \n",
        "\n",
        "### True for episodic setting, Noisy unbaised estimate.\n",
        "\n",
        "### TD-Learning Update:\n",
        "\n",
        "### Uses Bootstrap.\n",
        "\n",
        "### We sample ((s1, r1, s'1),(s2, r2, s'2),.......(sn, rn, s'n))... for batch training\n",
        "\n",
        "## W = W + alpha * ((rt + gamma*V(s't :w)) - V(st :w)).dV'(st:w)/dw...............(3)\n",
        "\n",
        "### W = W + alpha * ((rt + gamma* transpose(X(s't)).W) - transpose(X(st)).W).dV'(st:w)/dw \n",
        "\n",
        "### W = W + alpha * ((rt + gamma* transpose(X(s't)).W) - transpose(X(st)).W).X(st)......(4)\n",
        "\n",
        "### True for non-episodic setting, baised estimate, no variance.\n",
        "\n",
        "#### FOR STATE-ACTION VALUE FUNCTION:\n",
        "\n",
        "### Monte-Carlo Update:\n",
        "\n",
        "### We sample ((s1, a1, G1), (s2, a2, G2).................(sn, an, Gn))... for batch training\n",
        "\n",
        "## W = W + alpha * (Gt - Q'(st, at : w))dQ'(st, at :w)/dw.................(1)\n",
        "\n",
        "### SARSA Update:\n",
        "\n",
        "### Uses Bootstrap.\n",
        "\n",
        "### We sample ((s1, a1, r1, s'1, a'1),(s2, a2, r2, s'2, a_2),.......(sn, an, rn, s'n, a'n))... for batch training\n",
        "\n",
        "## W = W + alpha * ((rt + gamma*Q'(s't,a't :w)) - Q'(st,at :w)).dQ'(st,at :w)/dw...............(2)\n",
        "\n",
        "### Q-learning Update:\n",
        "\n",
        "### Uses Bootstrap, Optimistic policy\n",
        "\n",
        "### We sample ((s1, a1, r1, s'1),(s2, a2, r2, s'2),.......(sn, an, rn, s'n))... for batch training\n",
        "\n",
        "## W = W + alpha * ((rt + gamma* max(Q'(s't, a :w))) - Q'(st,at :w)).dQ'(st,at :w)/dw...............(3)\n",
        "### a = all actions in action space. So, the maximum value of all actions for that state.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXS7wgKyaf8w"
      },
      "source": [
        "### We use the Q learning based oracle in the DQN. \n",
        "\n",
        "### The maximization bias exists as Q-learning uses a optimistic policy.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z40U4pI1cwJm"
      },
      "source": [
        "### The above equation is for Linear VFA: V(s)=transponse(X(s)).W\n",
        "\n",
        "### The linear VFA may have a some constraint, if the value function is non-linear in nature.\n",
        "### So, we use a Deep Learning, which can replicate non-linear as well as linear functions.\n",
        "\n",
        "### V(s)=g1(B1+ W1. transpose(g0(B0 + W0.transpose(X(s)))))\n",
        "\n",
        "### where V(s) is the value function for state s. X(s) is the feature set, sent to a 2 layered Neural Network.\n",
        "\n",
        "### g0, g1 are activation functions, W0, W1 and B0, B1 are the weights and biases of the layers of the Neural Networks.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSGVh6xHgcoX"
      },
      "source": [
        "### There are some problems using Neural Networks based VFA.\n",
        "\n",
        "### 1. In case of Linear VFA has less correlations among data so it gives an unbaised estimate, but for Non-linear VFA using Neural Networks\n",
        "### has a high degree correlations, so it may cause a biased learning process, so it will start performing better for the actions and states \n",
        "### it has seen in maximum cases. So, to prevent this from happening, DQN uses an Experience Replay Buffer to reduce bias.\n",
        "\n",
        "### The Prior experiences are stored in a buffer. (s,a,r,s') is stored in the buffer. \n",
        "\n",
        "### The buffer is of a fixed size of 10k or 1M samples,, which are either the best or the recet samples, mostly the lattter is used.\n",
        "\n",
        "### The idea is as we udate using different states, from experience, to prevent multiple occurence of the states.\n",
        "\n",
        "### 2. As we have seen the in case of the Q learning VFA based update:\n",
        "\n",
        "### W= W - alpha * (r + gamma* max(Q'(s',a :W))- (Q(s,a :W))).dQ'(s,a :w)/dw\n",
        "\n",
        "### we can see that, the oracle and the training network has the same parameters. So, when the training network is updated\n",
        "### the oracle also shifts, so the model tries to achieve a moving target, which creates instability,\n",
        "\n",
        "### for the oracle we use a different parameterized network, and for the trainee we use a different. \n",
        "\n",
        "### After a few training epochs we update the oracle with the trainee networks weights\n",
        "\n",
        "## This makes the target fixed and increases stability.\n",
        "\n",
        "\n",
        "### W= W - alpha * (r + gamma* max(Q'(s',a :W'))- (Q(s,a :W))).dQ'(s,a :W)/dW\n",
        "\n",
        "## W is the training parameter, W' is the oracle parameter.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x694u8b33gM"
      },
      "source": [
        "### DQN on Cart-Pole environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg3nVD6A1DiR"
      },
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from memory_module import replayBuffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DEyLiKU5AV6"
      },
      "source": [
        "#### instantiating environment\n",
        "#### instantiating replay-buffer to 100k samples size\n",
        "\n",
        "env=gym.make('CartPole-v0')\n",
        "env._max_episode_steps=400\n",
        "memory=replayBuffer(100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CJcGoXu9VMd"
      },
      "source": [
        "class DQN:\n",
        "\n",
        "  def __init__(self,env,buffer):\n",
        "    self.env=env\n",
        "    self.buffer=buffer    ### Replay buffer \n",
        "    self.state_dimension=env.observation_space.shape   ### Input state dimension\n",
        "    self.no_of_action=env.action_space.n              ### No of actions\n",
        "    self.learning_rate=0.01\n",
        "    self.gamma=0.99\n",
        "    self.optimizer=tf.keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
        "    self.train_DQN=None         #### Tranining network\n",
        "    self.fixed_DQN=None         #### Oracle network\n",
        " \n",
        "  def get_model(self):\n",
        "    ### Q = f(s,a: w)\n",
        "\n",
        "    state_input=tf.keras.layers.Input(self.state_dimension,name=\"State_input\")  ### state input\n",
        "\n",
        "    action_input=tf.keras.layers.Input((self.no_of_action,),name=\"Action_input\") ### Action input\n",
        "\n",
        "    net=tf.keras.layers.Dense(256,activation='relu')(state_input)\n",
        "    net=tf.keras.layers.Dense(256,activation='relu')(net)\n",
        "    output=tf.keras.layers.Dense(self.no_of_action,name=\"function_out\")(net)\n",
        "\n",
        "    ### So, the model takes in the state representation as input and produces the Q values for the all the actions\n",
        "    ### Then for each action, given by: action 1: [0 1], the [0 1] is multiplied with the output of the model in form [a1,a2]\n",
        "    ### to get the output of corresponding to the action required. [a1, a2].[0, 1] = [0, a2]\n",
        "\n",
        "    Q_values=tf.multiply(output,action_input, name=\"masking_output\")\n",
        "\n",
        "    model=tf.keras.Model(inputs=[state_input,action_input],outputs=[Q_values],name=\"DQN\")\n",
        "\n",
        "    ### array of the Q values is the final output of the model.\n",
        "\n",
        "    model.compile(loss=\"mse\",optimizer=self.optimizer)\n",
        "\n",
        "    ### as we want to minimize (Q[s,a]-Q'[s,a : w])^2 we use MSE.\n",
        "\n",
        "    return model\n",
        "  \n",
        "  def update_fixed(self):\n",
        "    self.fixed_DQN.set_weights(self.train_DQN.get_weights())\n",
        "    ### We will need to update the target or fixed networks with the trainee networks weight \n",
        "    ### after a few epochs.\n",
        "\n",
        "  def get_epsilon(self,episode,steady_epsilon=0.01,steady_episode=100000):\n",
        "    #### Getting the epilon for the the greedy epsilon policy, \n",
        "\n",
        "    ### epsilon linearly decays till the steady step and then becomes constant\n",
        "    if episode>steady_episode:  ##If we are above the steady episode, we return a steady epsilon\n",
        "      return steady_epsilon\n",
        "    else:\n",
        "      slope=(steady_epsilon - 1)/(steady_episode - 0) \n",
        "      ### Line (1,0) to (steady_epsilon,steady_episode)\n",
        "\n",
        "      ### slope*episode will give us the decrease in the value of epsilon\n",
        "      ### To get the value we add 1 to the value so it is (1 - decrease), as epsilon starts from 1.\n",
        "      return slope*episode + 1\n",
        "\n",
        "  def get_action(self,state,epsilon):\n",
        "\n",
        "    if np.random.random()<epsilon:\n",
        "      return np.random.randint(self.no_of_action)\n",
        "      ### choosing random action with probability epsilon/|actions| for each.\n",
        "    else:\n",
        "      ### State is given in the shape: array([-0.0213599 , -0.03238987, -0.0356761 , -0.0347844 ])\n",
        "      ### as a 1D array, for each shape, we need to reshape it to provide a 2D array like:\n",
        "      ### array([[-0.0213599 , -0.03238987, -0.0356761 , -0.0347844 ]])\n",
        "      reshaped_state=state.reshape(1,-1)\n",
        "      \n",
        "      ### We need to pick the action which provides maximum action. To get all actions Q values, we need\n",
        "      ### to send 1 for all the actions. so in this case, the action input to the model should be: [1,1]\n",
        "\n",
        "      action_input=np.ones((1,self.no_of_action))\n",
        "      action_probs=self.train_DQN.predict([reshaped_state,action_input])\n",
        "\n",
        "      ### Action_probs has dimension 2: [[a1, a2]] as: array([[-0.00160907, -0.00242554]], dtype=float32)\n",
        "\n",
        "      ### We need to take the maximum of the of the results of the actions. so, we take np.argmax()\n",
        "      ### But we take on the axis=1 as: \n",
        "      ### in case there are mini-batches it is required to get the action for all the predictions.\n",
        "\n",
        "      ### array([[-0.00242554, -0.00160907]], dtype=float32) for this action \n",
        "      ### np.argmax(res_2,axis=0) => 1\n",
        "\n",
        "      ### array([[-0.00160907, -0.00242554],\n",
        "      ###  [-0.00242554, -0.00160907]], dtype=float32) -> for this prediction\n",
        "      ### np.argmax(res_2,axis=0) => 0   while,\n",
        "      ### np.argmax(res_2,axis=1) => [0,1], so we take on axis =1\n",
        "\n",
        "      optimal_action=np.argmax(action_probs,axis=1)[0]\n",
        "\n",
        "      return optimal_action\n",
        "\n",
        "  def on_batch(self,s,a,r,s_,not_done,gamma=0.99):\n",
        "\n",
        "    ### batch inputs\n",
        "    batch_size=s.shape[0]\n",
        "\n",
        "    ## if s is of dimension (50,4). 50 is the batch size.\n",
        "    ### as we know in q function, we take the maximum of the Q values for all the functions in the next_state.\n",
        "\n",
        "    ### same as get_Action function, but here already in shape (,4) no need to reshape.\n",
        "    ### the Q function is set using the target or fixed DQN.\n",
        "\n",
        "    action_probs=self.fixed_DQN.predict([s_,np.ones((batch_size,self.no_of_action))])\n",
        "    ## Now the Q target\n",
        "    q_targets= r + gamma*np.multiply(not_done,np.max(action_probs,axis=1))\n",
        "    ### Updated Q targets for all the states, and all the actions.\n",
        "    ### If done, not done=0, for that state, only the rewards are considered.\n",
        "\n",
        "    #### Q_targets is of the shape [v1, v2, v3.... vn]  ### where v1 is the q value updated, for that state.\n",
        "    ### but to train the network, we need it in format, [[0,v1],[v2,0]...] considering for 1st sample, action 1\n",
        "    ### was selected by the model, i.e, the value must be corresponding to the action for the state.\n",
        "\n",
        "    q_target_formatted=np.multiply(q_targets.reshape(-1,1),tf.keras.utils.to_categorical(a,self.no_of_action))\n",
        "    self.train_DQN.train_on_batch([s,tf.keras.utils.to_categorical(a,self.no_of_action)],q_target_formatted)\n",
        "    ### Training for the state on which the action is taken.\n",
        "  \n",
        "  def get_experience(self):\n",
        "\n",
        "    curr_state=self.env.reset()\n",
        "    for _ in range(50000):  \n",
        "      ### Creating 50k steps in experience to start the initial training\n",
        "\n",
        "      act=self.env.action_space.sample()   ### initially we randomly sample from the action space.\n",
        "      next_state,reward,done,_=self.env.step(act) ### Taking actions\n",
        "      self.buffer.push(curr_state,act,reward,next_state,not done)  ### Recording the details in buffer.\n",
        "\n",
        "      if done:\n",
        "        curr_state=self.env.reset()   ### If done is 1, environment is reset.\n",
        "      else:\n",
        "        curr_state=next_state        ### state is updated.\n",
        "    \n",
        "  def train(self):\n",
        "    self.train_DQN=self.get_model()\n",
        "    self.fixed_DQN=self.get_model()\n",
        "    self.get_experience()\n",
        "    ### All Initialization steps done\n",
        "    episode_reward=0\n",
        "    no_of_comp=0\n",
        "    curr_state=self.env.reset()\n",
        "    for step in range(1000000):\n",
        "      ### training on 1M steps\n",
        "      act=self.get_action(curr_state,self.get_epsilon(step))  #### getting action according to current epsilon, and state\n",
        "      next_state,reward,done,_=self.env.step(act) ### Taking the action\n",
        "      episode_reward+=reward  ## updating the reward for the step\n",
        "    \n",
        "      self.buffer.push(curr_state,act,reward,next_state,not done)  ### Pushing the details in the buffer.\n",
        "      ### Size of the buffer is fixed. so it works on LRU or first in first out policy.\n",
        "      \n",
        "      if done:\n",
        "\n",
        "        curr_state=self.env.reset()\n",
        "        if no_of_comp%50==0:\n",
        "          print('On step {}, no. of complete episodes {} episode reward {}'.format(step,no_of_comp,episode_reward))\n",
        "        episode_reward=0  ### Updating the reward to 0\n",
        "        no_of_comp+=1\n",
        "      \n",
        "      else:\n",
        "        curr_state=next_state\n",
        "\n",
        "      if step%5000==0:    ### after 5000 steps the fixed or target DQN is updated.\n",
        "        self.update_fixed()\n",
        "      \n",
        "      if step%4==0:    ### after training for 4 steps on the batch we sample new batch.\n",
        "        s,a,r,s_,nd=self.buffer.sample(32)\n",
        "        self.on_batch(s,a,r,s_,nd)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaOQ5wVW2DnJ",
        "outputId": "ea285bdd-95e7-4cfa-d1ac-3a81ff0e3cc9"
      },
      "source": [
        "dqn=DQN(env,memory)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM6jYNHC2NZS",
        "outputId": "268b439e-a356-4f45-8587-98ede1be6e42"
      },
      "source": [
        "dqn.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On step 15, no. of complete episodes 0 episode reward 16.0\n",
            "On step 1082, no. of complete episodes 50 episode reward 17.0\n",
            "On step 1974, no. of complete episodes 100 episode reward 13.0\n",
            "On step 3049, no. of complete episodes 150 episode reward 14.0\n",
            "On step 4216, no. of complete episodes 200 episode reward 9.0\n",
            "On step 5199, no. of complete episodes 250 episode reward 19.0\n",
            "On step 6204, no. of complete episodes 300 episode reward 18.0\n",
            "On step 7222, no. of complete episodes 350 episode reward 31.0\n",
            "On step 8174, no. of complete episodes 400 episode reward 18.0\n",
            "On step 9141, no. of complete episodes 450 episode reward 59.0\n",
            "On step 10067, no. of complete episodes 500 episode reward 21.0\n",
            "On step 11032, no. of complete episodes 550 episode reward 15.0\n",
            "On step 12071, no. of complete episodes 600 episode reward 15.0\n",
            "On step 13398, no. of complete episodes 650 episode reward 13.0\n",
            "On step 14638, no. of complete episodes 700 episode reward 10.0\n",
            "On step 16120, no. of complete episodes 750 episode reward 19.0\n",
            "On step 17349, no. of complete episodes 800 episode reward 26.0\n",
            "On step 19196, no. of complete episodes 850 episode reward 90.0\n",
            "On step 20817, no. of complete episodes 900 episode reward 54.0\n",
            "On step 22791, no. of complete episodes 950 episode reward 17.0\n",
            "On step 24796, no. of complete episodes 1000 episode reward 62.0\n",
            "On step 27201, no. of complete episodes 1050 episode reward 74.0\n",
            "On step 29568, no. of complete episodes 1100 episode reward 14.0\n",
            "On step 31964, no. of complete episodes 1150 episode reward 35.0\n",
            "On step 35062, no. of complete episodes 1200 episode reward 15.0\n",
            "On step 37741, no. of complete episodes 1250 episode reward 42.0\n",
            "On step 41683, no. of complete episodes 1300 episode reward 36.0\n",
            "On step 46198, no. of complete episodes 1350 episode reward 129.0\n",
            "On step 52127, no. of complete episodes 1400 episode reward 162.0\n",
            "On step 60613, no. of complete episodes 1450 episode reward 175.0\n",
            "On step 71326, no. of complete episodes 1500 episode reward 171.0\n",
            "On step 84458, no. of complete episodes 1550 episode reward 310.0\n",
            "On step 98576, no. of complete episodes 1600 episode reward 215.0\n",
            "On step 112854, no. of complete episodes 1650 episode reward 259.0\n",
            "On step 126319, no. of complete episodes 1700 episode reward 220.0\n",
            "On step 139021, no. of complete episodes 1750 episode reward 400.0\n",
            "On step 150688, no. of complete episodes 1800 episode reward 241.0\n",
            "On step 164398, no. of complete episodes 1850 episode reward 296.0\n",
            "On step 178404, no. of complete episodes 1900 episode reward 327.0\n",
            "On step 192252, no. of complete episodes 1950 episode reward 310.0\n",
            "On step 204071, no. of complete episodes 2000 episode reward 240.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA10YfcgaSTC"
      },
      "source": [
        "### in this we have used action replay buffer to gather experience and train. The buffer uses a LRU \n",
        "### or the most recent tuple stays, and the least recent is overwritten.\n",
        "\n",
        "### A more stable version is created using a prioritized experienced replay buffer, i.e, instead of randomly stacking\n",
        "### experience, it is a priority based experience replay buffer.\n",
        "\n",
        "### in this case, less the bootstrapping error like TD error, more is the priority of the sample, and more the priority,\n",
        "### more is the chance, the tuple stays in the buffer. \n",
        "\n",
        "### It is priority scheduling instead of LRU."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}